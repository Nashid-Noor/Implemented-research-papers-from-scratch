{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03: LoRA (Low-Rank Adaptation) from Scratch\n",
    "\n",
    "Deep learning paper implementation from scratch using PyTorch.\n",
    "- Dramatically reduces trainable parameters (often 10,000x fewer)\n",
    "- No additional inference latency (can merge weights)\n",
    "- Can switch between tasks by swapping LoRA weights\n",
    "1. LoRA Module Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "import copy\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. LoRA Module Implementation\n",
    "\n",
    "We implement a LoRA layer that wraps `nn.Linear`. The key components are:\n",
    "- **A matrix**: Initialized with Kaiming uniform (or Gaussian)\n",
    "- **B matrix**: Initialized to zero (so LoRA starts as identity)\n",
    "- **Scaling factor**: $\\alpha / r$ to control the magnitude of updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALinear(nn.Module):\n",
    "    def __init__(self, base_layer: nn.Linear, r: int = 4, alpha: float = 1.0, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.base_layer = base_layer\n",
    "        self.r = r\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / r\n",
    "        \n",
    "        in_features = base_layer.in_features\n",
    "        out_features = base_layer.out_features\n",
    "        \n",
    "        # LoRA matrices\n",
    "        # A: (r, in_features) - projects input to low-rank space\n",
    "        # B: (out_features, r) - projects back to output space\n",
    "        self.lora_A = nn.Parameter(torch.zeros(r, in_features))\n",
    "        self.lora_B = nn.Parameter(torch.zeros(out_features, r))\n",
    "        \n",
    "        # Optional dropout on LoRA path\n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
    "        \n",
    "        # Initialize A with Kaiming uniform, B with zeros\n",
    "        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_B)\n",
    "        \n",
    "        # Freeze base layer\n",
    "        for param in self.base_layer.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # For merge/unmerge\n",
    "        self.merged = False\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Base forward pass (frozen)\n",
    "        base_output = self.base_layer(x)\n",
    "        \n",
    "        if self.merged:\n",
    "            # If merged, base_layer already contains LoRA weights\n",
    "            return base_output\n",
    "        \n",
    "        # LoRA path: x @ A^T @ B^T * scaling\n",
    "        # x: (..., in_features)\n",
    "        # A: (r, in_features) -> x @ A^T: (..., r)\n",
    "        lora_output = self.dropout(x) @ self.lora_A.T @ self.lora_B.T * self.scaling\n",
    "        \n",
    "        return base_output + lora_output\n",
    "    \n",
    "    def merge(self):\n",
    "        if self.merged:\n",
    "            return\n",
    "        # W_merged = W_0 + B @ A * scaling\n",
    "        delta_W = (self.lora_B @ self.lora_A) * self.scaling\n",
    "        self.base_layer.weight.data += delta_W\n",
    "        self.merged = True\n",
    "    \n",
    "    def unmerge(self):\n",
    "        if not self.merged:\n",
    "            return\n",
    "        delta_W = (self.lora_B @ self.lora_A) * self.scaling\n",
    "        self.base_layer.weight.data -= delta_W\n",
    "        self.merged = False\n",
    "    \n",
    "    def get_lora_params(self):\n",
    "        return [self.lora_A, self.lora_B]\n",
    "    \n",
    "    @property\n",
    "    def lora_param_count(self):\n",
    "        return self.lora_A.numel() + self.lora_B.numel()\n",
    "    \n",
    "    @property\n",
    "    def base_param_count(self):\n",
    "        return sum(p.numel() for p in self.base_layer.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test of LoRALinear\n",
    "base_linear = nn.Linear(64, 128)\n",
    "lora_linear = LoRALinear(base_linear, r=4, alpha=1.0)\n",
    "\n",
    "x = torch.randn(2, 10, 64)  # (batch, seq, features)\n",
    "\n",
    "# Before training, LoRA should be near-identity (B is zeros)\n",
    "with torch.no_grad():\n",
    "    base_out = base_linear(x)\n",
    "    lora_out = lora_linear(x)\n",
    "    diff = (base_out - lora_out).abs().max().item()\n",
    "    print(f\"Max difference (should be ~0 since B=0): {diff:.2e}\")\n",
    "\n",
    "print(f\"Base params: {lora_linear.base_param_count:,}\")\n",
    "print(f\"LoRA params: {lora_linear.lora_param_count:,}\")\n",
    "print(f\"Compression ratio: {lora_linear.base_param_count / lora_linear.lora_param_count:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Small Transformer Language Model\n",
    "\n",
    "We'll create a small GPT-style transformer and then apply LoRA to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        \n",
    "        # These will be replaced with LoRA versions\n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_o = nn.Linear(d_model, d_model, bias=False)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = self.W_k(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = self.W_v(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        attn_output = torch.matmul(attn_weights, V)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        \n",
    "        return self.W_o(attn_output)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.fc2(self.dropout(F.gelu(self.fc1(x))))\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.ff = FeedForward(d_model, d_ff, dropout)\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        x = x + self.dropout(self.attention(self.ln1(x), mask))\n",
    "        x = x + self.dropout(self.ff(self.ln2(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class SmallGPT(nn.Module):\n",
    "    def __init__(self, vocab_size: int, d_model: int = 128, num_heads: int = 4, \n",
    "                 num_layers: int = 4, d_ff: int = 256, max_seq_len: int = 64, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.ln_final = nn.LayerNorm(d_model)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "        \n",
    "        # Weight tying\n",
    "        self.lm_head.weight = self.token_embedding.weight\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size, seq_len = x.shape\n",
    "        \n",
    "        positions = torch.arange(seq_len, device=x.device).unsqueeze(0)\n",
    "        \n",
    "        x = self.token_embedding(x) + self.position_embedding(positions)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Causal mask\n",
    "        mask = torch.tril(torch.ones(seq_len, seq_len, device=x.device)).unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            x = block(x, mask)\n",
    "        \n",
    "        x = self.ln_final(x)\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Apply LoRA to the Model\n",
    "\n",
    "We'll replace the linear layers in the attention mechanism (Q, K, V, O projections) with LoRA versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_lora_to_model(model: nn.Module, r: int = 4, alpha: float = 1.0, \n",
    "                        target_modules: list = None, dropout: float = 0.0) -> nn.Module:\n",
    "    if target_modules is None:\n",
    "        target_modules = ['W_q', 'W_k', 'W_v', 'W_o']  # Default: all attention projections\n",
    "    \n",
    "    lora_layers = []\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # Check if this module should have LoRA applied\n",
    "            should_apply = any(target in name for target in target_modules)\n",
    "            if should_apply:\n",
    "                # Get parent module and attribute name\n",
    "                parts = name.rsplit('.', 1)\n",
    "                if len(parts) == 2:\n",
    "                    parent_name, attr_name = parts\n",
    "                    parent = dict(model.named_modules())[parent_name]\n",
    "                else:\n",
    "                    parent = model\n",
    "                    attr_name = name\n",
    "                \n",
    "                # Create LoRA wrapper\n",
    "                lora_layer = LoRALinear(module, r=r, alpha=alpha, dropout=dropout)\n",
    "                setattr(parent, attr_name, lora_layer)\n",
    "                lora_layers.append((name, lora_layer))\n",
    "    \n",
    "    return model, lora_layers\n",
    "\n",
    "\n",
    "def get_lora_params(model: nn.Module):\n",
    "    lora_params = []\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, LoRALinear):\n",
    "            lora_params.extend(module.get_lora_params())\n",
    "    return lora_params\n",
    "\n",
    "\n",
    "def count_parameters(model: nn.Module):\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total, trainable\n",
    "\n",
    "\n",
    "def merge_lora(model: nn.Module):\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, LoRALinear):\n",
    "            module.merge()\n",
    "\n",
    "\n",
    "def unmerge_lora(model: nn.Module):\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, LoRALinear):\n",
    "            module.unmerge()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preparation\n",
    "\n",
    "We'll create a tiny corpus for fine-tuning. The base model will be \"pretrained\" on general text, then fine-tuned with LoRA on a specific style/domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple tokenizer\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self):\n",
    "        self.special_tokens = ['[PAD]', '[UNK]', '[BOS]', '[EOS]']\n",
    "        self.word2idx = {tok: i for i, tok in enumerate(self.special_tokens)}\n",
    "        self.idx2word = {i: tok for i, tok in enumerate(self.special_tokens)}\n",
    "        self.vocab_size = len(self.special_tokens)\n",
    "    \n",
    "    def fit(self, texts: list):\n",
    "        for text in texts:\n",
    "            for word in text.lower().split():\n",
    "                word = ''.join(c for c in word if c.isalnum())\n",
    "                if word and word not in self.word2idx:\n",
    "                    self.word2idx[word] = self.vocab_size\n",
    "                    self.idx2word[self.vocab_size] = word\n",
    "                    self.vocab_size += 1\n",
    "    \n",
    "    def encode(self, text: str) -> list:\n",
    "        tokens = [self.word2idx['[BOS]']]\n",
    "        for word in text.lower().split():\n",
    "            word = ''.join(c for c in word if c.isalnum())\n",
    "            if word:\n",
    "                tokens.append(self.word2idx.get(word, self.word2idx['[UNK]']))\n",
    "        tokens.append(self.word2idx['[EOS]'])\n",
    "        return tokens\n",
    "    \n",
    "    def decode(self, tokens: list) -> str:\n",
    "        words = []\n",
    "        for t in tokens:\n",
    "            word = self.idx2word.get(t, '[UNK]')\n",
    "            if word not in ['[PAD]', '[BOS]', '[EOS]']:\n",
    "                words.append(word)\n",
    "        return ' '.join(words)\n",
    "\n",
    "\n",
    "# Pretraining corpus (general text)\n",
    "pretrain_corpus = [\n",
    "    \"The cat sat on the mat and looked around the room.\",\n",
    "    \"A dog ran through the park chasing a ball.\",\n",
    "    \"The sun was shining brightly in the clear blue sky.\",\n",
    "    \"She walked to the store to buy some groceries.\",\n",
    "    \"The book was lying on the table near the window.\",\n",
    "    \"He played the piano beautifully at the concert.\",\n",
    "    \"The flowers in the garden were blooming nicely.\",\n",
    "    \"They watched a movie together on the weekend.\",\n",
    "    \"The train arrived at the station on time.\",\n",
    "    \"She made a delicious cake for the birthday party.\",\n",
    "    \"The birds were singing in the trees early morning.\",\n",
    "    \"He fixed the broken chair with some tools.\",\n",
    "    \"The rain started falling heavily in the afternoon.\",\n",
    "    \"They played football in the field after school.\",\n",
    "    \"The teacher explained the lesson clearly to students.\",\n",
    "] * 20  # Repeat for more data\n",
    "\n",
    "# Fine-tuning corpus (specific style - formal/technical)\n",
    "finetune_corpus = [\n",
    "    \"The experiment demonstrated significant improvements in accuracy.\",\n",
    "    \"Our analysis reveals a strong correlation between variables.\",\n",
    "    \"The methodology employed rigorous statistical techniques.\",\n",
    "    \"Results indicate substantial performance gains across metrics.\",\n",
    "    \"The framework provides a systematic approach to problem solving.\",\n",
    "    \"Implementation details are described in the following section.\",\n",
    "    \"The proposed method outperforms existing baselines significantly.\",\n",
    "    \"Experimental validation confirms the theoretical predictions.\",\n",
    "    \"The algorithm achieves state of the art results.\",\n",
    "    \"Further research is needed to explore these findings.\",\n",
    "] * 30  # Repeat for more data\n",
    "\n",
    "# Build tokenizer\n",
    "tokenizer = SimpleTokenizer()\n",
    "tokenizer.fit(pretrain_corpus + finetune_corpus)\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMDataset(Dataset):\n",
    "    def __init__(self, texts: list, tokenizer: SimpleTokenizer, max_len: int = 32):\n",
    "        self.data = []\n",
    "        for text in texts:\n",
    "            tokens = tokenizer.encode(text)\n",
    "            if len(tokens) > max_len:\n",
    "                tokens = tokens[:max_len]\n",
    "            else:\n",
    "                tokens = tokens + [tokenizer.word2idx['[PAD]']] * (max_len - len(tokens))\n",
    "            self.data.append(torch.tensor(tokens))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.data[idx]\n",
    "        return tokens[:-1], tokens[1:]\n",
    "\n",
    "# Create datasets\n",
    "pretrain_dataset = LMDataset(pretrain_corpus, tokenizer, max_len=32)\n",
    "finetune_dataset = LMDataset(finetune_corpus, tokenizer, max_len=32)\n",
    "\n",
    "pretrain_loader = DataLoader(pretrain_dataset, batch_size=16, shuffle=True)\n",
    "finetune_loader = DataLoader(finetune_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "print(f\"Pretrain samples: {len(pretrain_dataset)}\")\n",
    "print(f\"Finetune samples: {len(finetune_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, device, pad_idx=0):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    for inputs, targets in dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(inputs)\n",
    "        \n",
    "        # Compute loss (ignore padding)\n",
    "        loss = F.cross_entropy(\n",
    "            logits.view(-1, logits.size(-1)),\n",
    "            targets.view(-1),\n",
    "            ignore_index=pad_idx\n",
    "        )\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Count non-pad tokens\n",
    "        mask = targets != pad_idx\n",
    "        total_loss += loss.item() * mask.sum().item()\n",
    "        total_tokens += mask.sum().item()\n",
    "    \n",
    "    return total_loss / total_tokens if total_tokens > 0 else 0\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, device, pad_idx=0):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            logits = model(inputs)\n",
    "            \n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(-1, logits.size(-1)),\n",
    "                targets.view(-1),\n",
    "                ignore_index=pad_idx,\n",
    "                reduction='none'\n",
    "            )\n",
    "            \n",
    "            mask = targets.view(-1) != pad_idx\n",
    "            total_loss += (loss * mask).sum().item()\n",
    "            total_tokens += mask.sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / total_tokens if total_tokens > 0 else 0\n",
    "    perplexity = math.exp(avg_loss) if avg_loss < 100 else float('inf')\n",
    "    return avg_loss, perplexity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Pretrain Base Model\n",
    "\n",
    "First, we'll \"pretrain\" the base model on general text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create base model\n",
    "base_model = SmallGPT(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    d_model=128,\n",
    "    num_heads=4,\n",
    "    num_layers=4,\n",
    "    d_ff=256,\n",
    "    max_seq_len=32,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "total_params, trainable_params = count_parameters(base_model)\n",
    "print(f\"Base model - Total params: {total_params:,}, Trainable: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretrain\n",
    "print(\"Pretraining base model...\")\n",
    "optimizer = torch.optim.AdamW(base_model.parameters(), lr=1e-3, weight_decay=0.01)\n",
    "\n",
    "pretrain_losses = []\n",
    "for epoch in range(10):\n",
    "    loss = train_epoch(base_model, pretrain_loader, optimizer, device)\n",
    "    pretrain_losses.append(loss)\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        _, ppl = evaluate(base_model, pretrain_loader, device)\n",
    "        print(f\"Epoch {epoch+1}: Loss = {loss:.4f}, Perplexity = {ppl:.2f}\")\n",
    "\n",
    "print(\"Pretraining complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save base model weights for comparison later\n",
    "base_model_state = copy.deepcopy(base_model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Apply LoRA and Fine-tune\n",
    "\n",
    "Now we apply LoRA to the attention projections and fine-tune only the LoRA parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LoRA to attention projections\n",
    "lora_model, lora_layers = apply_lora_to_model(\n",
    "    base_model, \n",
    "    r=4, \n",
    "    alpha=8.0,  # Common practice: alpha = 2*r\n",
    "    target_modules=['W_q', 'W_v'],  # Only Q and V projections (common in practice)\n",
    "    dropout=0.05\n",
    ")\n",
    "\n",
    "print(f\"Applied LoRA to {len(lora_layers)} layers:\")\n",
    "for name, layer in lora_layers:\n",
    "    print(f\"  - {name}: r={layer.r}, base_params={layer.base_param_count:,}, lora_params={layer.lora_param_count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter count comparison\n",
    "total_params, trainable_params = count_parameters(lora_model)\n",
    "lora_only_params = sum(p.numel() for p in get_lora_params(lora_model))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PARAMETER COUNT COMPARISON\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total model parameters:      {total_params:>10,}\")\n",
    "print(f\"Full fine-tuning params:     {total_params:>10,}\")\n",
    "print(f\"LoRA trainable params:       {lora_only_params:>10,}\")\n",
    "print(f\"Reduction ratio:             {total_params / lora_only_params:>10.1f}x\")\n",
    "print(f\"% of original:               {100 * lora_only_params / total_params:>10.2f}%\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune with LoRA (only LoRA params)\n",
    "print(\"\\nFine-tuning with LoRA...\")\n",
    "lora_params = get_lora_params(lora_model)\n",
    "optimizer = torch.optim.AdamW(lora_params, lr=1e-3, weight_decay=0.01)\n",
    "\n",
    "finetune_losses = []\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(15):\n",
    "    loss = train_epoch(lora_model, finetune_loader, optimizer, device)\n",
    "    finetune_losses.append(loss)\n",
    "    if (epoch + 1) % 3 == 0:\n",
    "        _, ppl = evaluate(lora_model, finetune_loader, device)\n",
    "        print(f\"Epoch {epoch+1}: Loss = {loss:.4f}, Perplexity = {ppl:.2f}\")\n",
    "\n",
    "lora_finetune_time = time.time() - start_time\n",
    "print(f\"\\nLoRA fine-tuning time: {lora_finetune_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test merge/unmerge\n",
    "print(\"Testing merge/unmerge...\")\n",
    "\n",
    "lora_model.eval()\n",
    "test_input = torch.randint(0, tokenizer.vocab_size, (2, 16)).to(device)\n",
    "\n",
    "# Output before merge (with LoRA path)\n",
    "with torch.no_grad():\n",
    "    output_before_merge = lora_model(test_input).clone()\n",
    "\n",
    "# Merge LoRA weights\n",
    "merge_lora(lora_model)\n",
    "\n",
    "# Output after merge (LoRA baked into weights)\n",
    "with torch.no_grad():\n",
    "    output_after_merge = lora_model(test_input).clone()\n",
    "\n",
    "# Check difference\n",
    "max_diff = (output_before_merge - output_after_merge).abs().max().item()\n",
    "print(f\"Max difference after merge: {max_diff:.2e}\")\n",
    "print(f\"Merge preserves outputs: {max_diff < 1e-5}\")\n",
    "\n",
    "# Unmerge\n",
    "unmerge_lora(lora_model)\n",
    "\n",
    "# Output after unmerge\n",
    "with torch.no_grad():\n",
    "    output_after_unmerge = lora_model(test_input).clone()\n",
    "\n",
    "max_diff_unmerge = (output_before_merge - output_after_unmerge).abs().max().item()\n",
    "print(f\"Max difference after unmerge: {max_diff_unmerge:.2e}\")\n",
    "print(f\"Unmerge restores original: {max_diff_unmerge < 1e-5}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Ablation: Rank r=4 vs r=16\n",
    "\n",
    "Compare different LoRA ranks in terms of performance and training speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lora_experiment(rank: int, alpha: float = None):\n",
    "    if alpha is None:\n",
    "        alpha = 2 * rank  # Common heuristic\n",
    "    \n",
    "    # Create fresh model and load pretrained weights\n",
    "    model = SmallGPT(\n",
    "        vocab_size=tokenizer.vocab_size,\n",
    "        d_model=128,\n",
    "        num_heads=4,\n",
    "        num_layers=4,\n",
    "        d_ff=256,\n",
    "        max_seq_len=32,\n",
    "        dropout=0.1\n",
    "    ).to(device)\n",
    "    model.load_state_dict(base_model_state)\n",
    "    \n",
    "    # Apply LoRA\n",
    "    model, lora_layers = apply_lora_to_model(\n",
    "        model, r=rank, alpha=alpha, target_modules=['W_q', 'W_v']\n",
    "    )\n",
    "    \n",
    "    # Count parameters\n",
    "    lora_param_count = sum(p.numel() for p in get_lora_params(model))\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(get_lora_params(model), lr=1e-3, weight_decay=0.01)\n",
    "    \n",
    "    losses = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(15):\n",
    "        loss = train_epoch(model, finetune_loader, optimizer, device)\n",
    "        losses.append(loss)\n",
    "    \n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # Final evaluation\n",
    "    final_loss, final_ppl = evaluate(model, finetune_loader, device)\n",
    "    \n",
    "    return {\n",
    "        'rank': rank,\n",
    "        'alpha': alpha,\n",
    "        'lora_params': lora_param_count,\n",
    "        'final_loss': final_loss,\n",
    "        'final_ppl': final_ppl,\n",
    "        'train_time': train_time,\n",
    "        'losses': losses\n",
    "    }\n",
    "\n",
    "print(\"Running ablation study...\\n\")\n",
    "\n",
    "# Run experiments\n",
    "results_r4 = run_lora_experiment(rank=4)\n",
    "print(f\"Rank 4: Loss={results_r4['final_loss']:.4f}, PPL={results_r4['final_ppl']:.2f}, Time={results_r4['train_time']:.2f}s\")\n",
    "\n",
    "results_r16 = run_lora_experiment(rank=16)\n",
    "print(f\"Rank 16: Loss={results_r16['final_loss']:.4f}, PPL={results_r16['final_ppl']:.2f}, Time={results_r16['train_time']:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ablation results comparison\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ABLATION STUDY: LoRA Rank Comparison\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Metric':<25} {'Rank=4':<20} {'Rank=16':<20}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'LoRA Parameters':<25} {results_r4['lora_params']:>15,} {results_r16['lora_params']:>15,}\")\n",
    "print(f\"{'Final Loss':<25} {results_r4['final_loss']:>15.4f} {results_r16['final_loss']:>15.4f}\")\n",
    "print(f\"{'Final Perplexity':<25} {results_r4['final_ppl']:>15.2f} {results_r16['final_ppl']:>15.2f}\")\n",
    "print(f\"{'Training Time (s)':<25} {results_r4['train_time']:>15.2f} {results_r16['train_time']:>15.2f}\")\n",
    "print(f\"{'Params/Perf Ratio':<25} {results_r4['lora_params']/results_r4['final_ppl']:>15.1f} {results_r16['lora_params']/results_r16['final_ppl']:>15.1f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Loss curves\n",
    "axes[0].plot(results_r4['losses'], label='Rank=4', marker='o', markersize=3)\n",
    "axes[0].plot(results_r16['losses'], label='Rank=16', marker='s', markersize=3)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss by LoRA Rank')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Parameter comparison bar chart\n",
    "total_params = count_parameters(base_model)[0]\n",
    "x = ['Full\\nFine-tuning', 'LoRA\\nRank=4', 'LoRA\\nRank=16']\n",
    "y = [total_params, results_r4['lora_params'], results_r16['lora_params']]\n",
    "colors = ['#ff6b6b', '#4ecdc4', '#45b7d1']\n",
    "\n",
    "bars = axes[1].bar(x, y, color=colors)\n",
    "axes[1].set_ylabel('Trainable Parameters')\n",
    "axes[1].set_title('Parameter Efficiency')\n",
    "axes[1].set_yscale('log')\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars, y):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height(), \n",
    "                 f'{val:,}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Text Generation Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, prompt: str, max_len: int = 20, temperature: float = 0.8):\n",
    "    model.eval()\n",
    "    tokens = tokenizer.encode(prompt)[:-1]  # Remove EOS\n",
    "    tokens = torch.tensor(tokens).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len):\n",
    "            if tokens.size(1) >= model.max_seq_len:\n",
    "                break\n",
    "            \n",
    "            logits = model(tokens)\n",
    "            next_token_logits = logits[0, -1, :] / temperature\n",
    "            probs = F.softmax(next_token_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, 1)\n",
    "            \n",
    "            if next_token.item() == tokenizer.word2idx['[EOS]']:\n",
    "                break\n",
    "            \n",
    "            tokens = torch.cat([tokens, next_token.unsqueeze(0)], dim=1)\n",
    "    \n",
    "    return tokenizer.decode(tokens[0].tolist())\n",
    "\n",
    "base_only_model = SmallGPT(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    d_model=128,\n",
    "    num_heads=4,\n",
    "    num_layers=4,\n",
    "    d_ff=256,\n",
    "    max_seq_len=32,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "base_only_model.load_state_dict(base_model_state)\n",
    "\n",
    "print(\"Sample generations:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "prompts = [\"The experiment\", \"Our analysis\", \"The results\"]\n",
    "for prompt in prompts:\n",
    "    print(f\"\\nPrompt: '{prompt}'\")\n",
    "    print(f\"  Base model:  {generate_text(base_only_model, tokenizer, prompt)}\")\n",
    "    print(f\"  LoRA model:  {generate_text(lora_model, tokenizer, prompt)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}