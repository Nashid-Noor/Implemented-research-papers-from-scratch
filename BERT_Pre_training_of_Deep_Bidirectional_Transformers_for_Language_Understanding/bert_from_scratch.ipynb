{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - BERT MLM from Scratch\n",
    "\n",
    "Deep learning paper implementation from scratch using PyTorch.\n",
    "1. **Simple Word-Level Tokenizer** - With special tokens [PAD], [UNK], [CLS], [SEP], [MASK]\n",
    "- Mask probability: 0.15 vs 0.20\n",
    "- Number of layers: 2 vs 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import time\n",
    "from typing import Optional, Tuple, List, Dict, Any\n",
    "from dataclasses import dataclass\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "set_seed(42)\n",
    "\n",
    "# Deterministic settings\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BERTConfig:\n",
    "    # Model architecture\n",
    "    d_model: int = 256           # Hidden size\n",
    "    n_heads: int = 8             # Number of attention heads\n",
    "    n_layers: int = 4            # Number of encoder layers\n",
    "    d_ff: int = 512              # Feed-forward intermediate size\n",
    "    max_seq_len: int = 128       # Maximum sequence length\n",
    "    vocab_size: int = 10000      # Will be updated after building tokenizer\n",
    "    n_segments: int = 2          # Number of segment types (sentence A/B)\n",
    "    dropout: float = 0.1         # Dropout rate\n",
    "    \n",
    "    # MLM settings\n",
    "    mask_prob: float = 0.15      # Probability of masking a token\n",
    "    \n",
    "    # Training\n",
    "    batch_size: int = 32\n",
    "    learning_rate: float = 1e-4\n",
    "    n_epochs: int = 10\n",
    "    warmup_steps: int = 100\n",
    "    \n",
    "config = BERTConfig()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Tokenizer Implementation\n",
    "\n",
    "We implement a simple word-level tokenizer. While BPE (Byte Pair Encoding) is more common in practice, word-level tokenization is simpler to implement and sufficient for demonstration.\n",
    "\n",
    "### Trade-offs:\n",
    "- **Word-level** (our choice):\n",
    "  - \u2705 Simple to implement\n",
    "  - \u2705 Words are semantically meaningful units\n",
    "  - \u274c Large vocabulary for good coverage\n",
    "  - \u274c Cannot handle OOV (out-of-vocabulary) words well\n",
    "  \n",
    "- **BPE / WordPiece**:\n",
    "  - \u2705 Smaller vocabulary\n",
    "  - \u2705 Better OOV handling (breaks into subwords)\n",
    "  - \u2705 Language-agnostic\n",
    "  - \u274c More complex to implement\n",
    "  - \u274c Subwords may not be semantically meaningful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTTokenizer:\n",
    "    \n",
    "    PAD_TOKEN = \"[PAD]\"\n",
    "    UNK_TOKEN = \"[UNK]\"\n",
    "    CLS_TOKEN = \"[CLS]\"\n",
    "    SEP_TOKEN = \"[SEP]\"\n",
    "    MASK_TOKEN = \"[MASK]\"\n",
    "    \n",
    "    def __init__(self, min_freq: int = 2, max_vocab_size: Optional[int] = None):\n",
    "        self.min_freq = min_freq\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        \n",
    "        self.word2idx: Dict[str, int] = {}\n",
    "        self.idx2word: Dict[int, str] = {}\n",
    "        \n",
    "        # Special token indices\n",
    "        self.special_tokens = [\n",
    "            self.PAD_TOKEN, self.UNK_TOKEN, self.CLS_TOKEN,\n",
    "            self.SEP_TOKEN, self.MASK_TOKEN\n",
    "        ]\n",
    "        \n",
    "    @property\n",
    "    def pad_idx(self) -> int:\n",
    "        return self.word2idx[self.PAD_TOKEN]\n",
    "    \n",
    "    @property\n",
    "    def unk_idx(self) -> int:\n",
    "        return self.word2idx[self.UNK_TOKEN]\n",
    "    \n",
    "    @property\n",
    "    def cls_idx(self) -> int:\n",
    "        return self.word2idx[self.CLS_TOKEN]\n",
    "    \n",
    "    @property\n",
    "    def sep_idx(self) -> int:\n",
    "        return self.word2idx[self.SEP_TOKEN]\n",
    "    \n",
    "    @property\n",
    "    def mask_idx(self) -> int:\n",
    "        return self.word2idx[self.MASK_TOKEN]\n",
    "    \n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        return len(self.word2idx)\n",
    "    \n",
    "    def fit(self, texts: List[str]) -> None:\n",
    "        # Count word frequencies\n",
    "        word_freq = Counter()\n",
    "        for text in texts:\n",
    "            words = self._tokenize(text)\n",
    "            word_freq.update(words)\n",
    "        \n",
    "        # Add special tokens first\n",
    "        for i, token in enumerate(self.special_tokens):\n",
    "            self.word2idx[token] = i\n",
    "            self.idx2word[i] = token\n",
    "        \n",
    "        # Add words that meet frequency threshold\n",
    "        idx = len(self.special_tokens)\n",
    "        sorted_words = sorted(word_freq.items(), key=lambda x: (-x[1], x[0]))\n",
    "        \n",
    "        for word, freq in sorted_words:\n",
    "            if freq < self.min_freq:\n",
    "                continue\n",
    "            if self.max_vocab_size and idx >= self.max_vocab_size:\n",
    "                break\n",
    "            \n",
    "            self.word2idx[word] = idx\n",
    "            self.idx2word[idx] = word\n",
    "            idx += 1\n",
    "            \n",
    "        print(f\"Vocabulary built: {self.vocab_size} tokens\")\n",
    "        print(f\"  - Special tokens: {len(self.special_tokens)}\")\n",
    "        print(f\"  - Regular tokens: {self.vocab_size - len(self.special_tokens)}\")\n",
    "        \n",
    "    def _tokenize(self, text: str) -> List[str]:\n",
    "        # Remove punctuation and lowercase\n",
    "        text = text.lower()\n",
    "        # Keep only alphanumeric and spaces\n",
    "        text = ''.join(c if c.isalnum() or c.isspace() else ' ' for c in text)\n",
    "        return text.split()\n",
    "    \n",
    "    def encode(\n",
    "        self,\n",
    "        text: str,\n",
    "        max_length: Optional[int] = None,\n",
    "        add_special_tokens: bool = True,\n",
    "        padding: bool = False\n",
    "    ) -> Dict[str, List[int]]:\n",
    "        words = self._tokenize(text)\n",
    "        \n",
    "        # Convert to indices\n",
    "        token_ids = [self.word2idx.get(w, self.unk_idx) for w in words]\n",
    "        \n",
    "        # Add special tokens\n",
    "        if add_special_tokens:\n",
    "            token_ids = [self.cls_idx] + token_ids + [self.sep_idx]\n",
    "        \n",
    "        # Truncate if necessary\n",
    "        if max_length and len(token_ids) > max_length:\n",
    "            token_ids = token_ids[:max_length]\n",
    "            if add_special_tokens:\n",
    "                token_ids[-1] = self.sep_idx  # Ensure [SEP] at end\n",
    "        \n",
    "        attention_mask = [1] * len(token_ids)\n",
    "        \n",
    "        # Segment IDs (all 0 for single sentence)\n",
    "        token_type_ids = [0] * len(token_ids)\n",
    "        \n",
    "        # Pad if necessary\n",
    "        if padding and max_length:\n",
    "            pad_len = max_length - len(token_ids)\n",
    "            token_ids = token_ids + [self.pad_idx] * pad_len\n",
    "            attention_mask = attention_mask + [0] * pad_len\n",
    "            token_type_ids = token_type_ids + [0] * pad_len\n",
    "        \n",
    "        return {\n",
    "            'input_ids': token_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'token_type_ids': token_type_ids\n",
    "        }\n",
    "    \n",
    "    def decode(self, token_ids: List[int], skip_special_tokens: bool = True) -> str:\n",
    "        words = []\n",
    "        for idx in token_ids:\n",
    "            word = self.idx2word.get(idx, self.UNK_TOKEN)\n",
    "            if skip_special_tokens and word in self.special_tokens:\n",
    "                continue\n",
    "            words.append(word)\n",
    "        return ' '.join(words)\n",
    "    \n",
    "    def get_random_token(self) -> int:\n",
    "        return random.randint(len(self.special_tokens), self.vocab_size - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample corpus for training\n",
    "SAMPLE_TEXTS = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"A journey of a thousand miles begins with a single step.\",\n",
    "    \"To be or not to be that is the question.\",\n",
    "    \"All that glitters is not gold.\",\n",
    "    \"The only thing we have to fear is fear itself.\",\n",
    "    \"In the beginning was the word and the word was with god.\",\n",
    "    \"It was the best of times it was the worst of times.\",\n",
    "    \"Call me ishmael some years ago never mind how long precisely.\",\n",
    "    \"It is a truth universally acknowledged that a single man in possession of a good fortune must be in want of a wife.\",\n",
    "    \"Happy families are all alike every unhappy family is unhappy in its own way.\",\n",
    "    \"The sun rose slowly over the mountains casting long shadows across the valley below.\",\n",
    "    \"She walked through the forest listening to the birds singing their morning songs.\",\n",
    "    \"The old man sat by the fire remembering the days of his youth.\",\n",
    "    \"The city streets were busy with people hurrying to their destinations.\",\n",
    "    \"A gentle breeze blew through the open window bringing the scent of flowers.\",\n",
    "    \"The children played in the garden while their parents watched from the porch.\",\n",
    "    \"He picked up the book and began to read losing himself in the story.\",\n",
    "    \"The stars twinkled in the night sky like diamonds scattered across velvet.\",\n",
    "    \"She smiled at the memory of their first meeting so many years ago.\",\n",
    "    \"The waves crashed against the shore creating a soothing rhythm.\",\n",
    "    \"Deep in the forest there lived a wise old owl who knew many secrets.\",\n",
    "    \"The train departed from the station carrying passengers to distant lands.\",\n",
    "    \"Music filled the air as the orchestra began their evening performance.\",\n",
    "    \"The scientist worked late into the night trying to solve the puzzle.\",\n",
    "    \"Rain began to fall gently at first then harder until it became a downpour.\",\n",
    "    \"The ancient temple stood silent witness to centuries of human history.\",\n",
    "    \"Birds migrated south as the leaves began to change colors in autumn.\",\n",
    "    \"The chef prepared an exquisite meal using fresh ingredients from the garden.\",\n",
    "    \"Lightning flashed across the sky followed by a thunderous roar.\",\n",
    "    \"The artist captured the essence of beauty in every brushstroke.\",\n",
    "]\n",
    "\n",
    "# Expand dataset by repetition\n",
    "TEXTS = SAMPLE_TEXTS * 100\n",
    "print(f\"Total texts: {len(TEXTS)}\")\n",
    "\n",
    "# Build tokenizer\n",
    "tokenizer = BERTTokenizer(min_freq=2)\n",
    "tokenizer.fit(TEXTS)\n",
    "\n",
    "# Update config\n",
    "config.vocab_size = tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test tokenizer\n",
    "test_text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "encoded = tokenizer.encode(test_text, max_length=20, padding=True)\n",
    "\n",
    "print(f\"Original: {test_text}\")\n",
    "print(f\"Token IDs: {encoded['input_ids']}\")\n",
    "print(f\"Attention mask: {encoded['attention_mask']}\")\n",
    "print(f\"Decoded: {tokenizer.decode(encoded['input_ids'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. MLM Dataset with Dynamic Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLMDataset(Dataset):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        texts: List[str],\n",
    "        tokenizer: BERTTokenizer,\n",
    "        max_length: int,\n",
    "        mask_prob: float = 0.15\n",
    "    ):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.mask_prob = mask_prob\n",
    "        \n",
    "        # Pre-tokenize all texts\n",
    "        self.examples = []\n",
    "        for text in texts:\n",
    "            encoded = tokenizer.encode(\n",
    "                text, \n",
    "                max_length=max_length, \n",
    "                add_special_tokens=True,\n",
    "                padding=True\n",
    "            )\n",
    "            self.examples.append(encoded)\n",
    "            \n",
    "        print(f\"Created {len(self.examples)} examples\")\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        example = self.examples[idx]\n",
    "        \n",
    "        input_ids = example['input_ids'].copy()\n",
    "        attention_mask = example['attention_mask'].copy()\n",
    "        token_type_ids = example['token_type_ids'].copy()\n",
    "        \n",
    "        # Labels: -100 means \"ignore\" in cross-entropy loss\n",
    "        labels = [-100] * len(input_ids)\n",
    "        masked_positions = [False] * len(input_ids)\n",
    "        \n",
    "        # Apply masking\n",
    "        for i, token_id in enumerate(input_ids):\n",
    "            # Don't mask special tokens or padding\n",
    "            if token_id in [self.tokenizer.pad_idx, self.tokenizer.cls_idx, \n",
    "                           self.tokenizer.sep_idx, self.tokenizer.mask_idx]:\n",
    "                continue\n",
    "            \n",
    "            # Randomly decide to mask this token\n",
    "            if random.random() < self.mask_prob:\n",
    "                # Store original label\n",
    "                labels[i] = token_id\n",
    "                masked_positions[i] = True\n",
    "                \n",
    "                # Decide how to corrupt\n",
    "                rand = random.random()\n",
    "                if rand < 0.8:\n",
    "                    # 80%: Replace with [MASK]\n",
    "                    input_ids[i] = self.tokenizer.mask_idx\n",
    "                elif rand < 0.9:\n",
    "                    # 10%: Replace with random token\n",
    "                    input_ids[i] = self.tokenizer.get_random_token()\n",
    "                # else: 10%: Keep original (already set)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'labels': torch.tensor(labels, dtype=torch.long),\n",
    "            'masked_positions': torch.tensor(masked_positions, dtype=torch.bool)\n",
    "        }\n",
    "\n",
    "# Create dataset\n",
    "mlm_dataset = MLMDataset(TEXTS, tokenizer, config.max_seq_len, config.mask_prob)\n",
    "mlm_dataloader = DataLoader(mlm_dataset, batch_size=config.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check: Verify masking ratio\n",
    "print(\"Verifying masking ratio...\")\n",
    "\n",
    "total_maskable = 0\n",
    "total_masked = 0\n",
    "\n",
    "for _ in range(100):\n",
    "    sample = mlm_dataset[random.randint(0, len(mlm_dataset)-1)]\n",
    "    # Count maskable tokens (non-special, non-padding)\n",
    "    maskable = (sample['labels'] != -100).sum().item() + \\\n",
    "               ((sample['attention_mask'] == 1) & (sample['labels'] == -100) & \n",
    "                (sample['input_ids'] != tokenizer.cls_idx) & \n",
    "                (sample['input_ids'] != tokenizer.sep_idx)).sum().item()\n",
    "    masked = (sample['labels'] != -100).sum().item()\n",
    "    \n",
    "    total_maskable += maskable\n",
    "    total_masked += masked\n",
    "\n",
    "observed_ratio = total_masked / total_maskable if total_maskable > 0 else 0\n",
    "print(f\"Expected mask ratio: {config.mask_prob:.2%}\")\n",
    "print(f\"Observed mask ratio: {observed_ratio:.2%}\")\n",
    "assert abs(observed_ratio - config.mask_prob) < 0.05, \"Masking ratio significantly off!\"\n",
    "print(\"\u2713 Masking ratio is within expected range\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. BERT Model Components\n",
    "\n",
    "### 8.1 Embeddings\n",
    "\n",
    "BERT uses three types of embeddings that are summed together:\n",
    "1. **Token embeddings**: Learned embedding for each vocabulary token\n",
    "2. **Position embeddings**: Learned embedding for each position\n",
    "3. **Segment embeddings**: Learned embedding for sentence A vs B (for NSP task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTEmbeddings(nn.Module):\n",
    "    \n",
    "    def __init__(self, config: BERTConfig):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Token embeddings\n",
    "        self.token_embeddings = nn.Embedding(\n",
    "            config.vocab_size, config.d_model, padding_idx=0\n",
    "        )\n",
    "        \n",
    "        # Position embeddings (learned, not sinusoidal)\n",
    "        self.position_embeddings = nn.Embedding(\n",
    "            config.max_seq_len, config.d_model\n",
    "        )\n",
    "        \n",
    "        # Segment embeddings (for sentence A/B)\n",
    "        self.segment_embeddings = nn.Embedding(\n",
    "            config.n_segments, config.d_model\n",
    "        )\n",
    "        \n",
    "        # Layer normalization and dropout\n",
    "        self.layer_norm = nn.LayerNorm(config.d_model, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "        # Register position IDs buffer\n",
    "        self.register_buffer(\n",
    "            'position_ids',\n",
    "            torch.arange(config.max_seq_len).unsqueeze(0)\n",
    "        )\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        token_type_ids: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        seq_len = input_ids.size(1)\n",
    "        \n",
    "        # Get position IDs\n",
    "        position_ids = self.position_ids[:, :seq_len]\n",
    "        \n",
    "        # Default segment IDs to 0\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros_like(input_ids)\n",
    "        \n",
    "        # Get embeddings\n",
    "        token_embeds = self.token_embeddings(input_ids)\n",
    "        position_embeds = self.position_embeddings(position_ids)\n",
    "        segment_embeds = self.segment_embeddings(token_type_ids)\n",
    "        \n",
    "        # Sum all embeddings\n",
    "        embeddings = token_embeds + position_embeds + segment_embeds\n",
    "        \n",
    "        # Apply layer norm and dropout\n",
    "        embeddings = self.layer_norm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "# Test embeddings\n",
    "embed = BERTEmbeddings(config)\n",
    "test_ids = torch.randint(0, config.vocab_size, (2, 20))\n",
    "test_seg = torch.zeros_like(test_ids)\n",
    "out = embed(test_ids, test_seg)\n",
    "assert out.shape == (2, 20, config.d_model)\n",
    "print(f\"Embeddings test passed. Output shape: {out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Multi-Head Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, config: BERTConfig):\n",
    "        super().__init__()\n",
    "        assert config.d_model % config.n_heads == 0\n",
    "        \n",
    "        self.d_model = config.d_model\n",
    "        self.n_heads = config.n_heads\n",
    "        self.d_head = config.d_model // config.n_heads\n",
    "        \n",
    "        # Linear projections\n",
    "        self.query = nn.Linear(config.d_model, config.d_model)\n",
    "        self.key = nn.Linear(config.d_model, config.d_model)\n",
    "        self.value = nn.Linear(config.d_model, config.d_model)\n",
    "        self.output = nn.Linear(config.d_model, config.d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Project to Q, K, V\n",
    "        Q = self.query(x)\n",
    "        K = self.key(x)\n",
    "        V = self.value(x)\n",
    "        \n",
    "        # Reshape to (batch, n_heads, seq_len, d_head)\n",
    "        Q = Q.view(batch_size, seq_len, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_head)\n",
    "        \n",
    "        # Apply attention mask\n",
    "        if attention_mask is not None:\n",
    "            # Expand mask to (batch, 1, 1, seq_len)\n",
    "            mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        # Softmax and dropout\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        attn_output = torch.matmul(attn_weights, V)\n",
    "        \n",
    "        # Reshape back\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(\n",
    "            batch_size, seq_len, self.d_model\n",
    "        )\n",
    "        \n",
    "        # Output projection\n",
    "        output = self.output(attn_output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Test attention\n",
    "attn = MultiHeadSelfAttention(config)\n",
    "x = torch.randn(2, 20, config.d_model)\n",
    "mask = torch.ones(2, 20)\n",
    "out = attn(x, mask)\n",
    "assert out.shape == x.shape\n",
    "print(f\"Multi-Head Attention test passed. Output shape: {out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Feed-Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \n",
    "    def __init__(self, config: BERTConfig):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(config.d_model, config.d_ff)\n",
    "        self.linear2 = nn.Linear(config.d_ff, config.d_model)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # BERT uses GELU activation instead of ReLU\n",
    "        return self.linear2(self.dropout(F.gelu(self.linear1(x))))\n",
    "\n",
    "# Test FFN\n",
    "ffn = FeedForward(config)\n",
    "x = torch.randn(2, 20, config.d_model)\n",
    "out = ffn(x)\n",
    "assert out.shape == x.shape\n",
    "print(f\"FFN test passed. Output shape: {out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTEncoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, config: BERTConfig):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadSelfAttention(config)\n",
    "        self.feed_forward = FeedForward(config)\n",
    "        self.norm1 = nn.LayerNorm(config.d_model, eps=1e-12)\n",
    "        self.norm2 = nn.LayerNorm(config.d_model, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        # Self-attention with residual + norm\n",
    "        attn_output = self.attention(x, attention_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        # FFN with residual + norm\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Test encoder layer\n",
    "enc_layer = BERTEncoderLayer(config)\n",
    "x = torch.randn(2, 20, config.d_model)\n",
    "out = enc_layer(x)\n",
    "assert out.shape == x.shape\n",
    "print(f\"Encoder layer test passed. Output shape: {out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5 MLM Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLMHead(nn.Module):\n",
    "    \n",
    "    def __init__(self, config: BERTConfig):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.d_model, config.d_model)\n",
    "        self.layer_norm = nn.LayerNorm(config.d_model, eps=1e-12)\n",
    "        self.decoder = nn.Linear(config.d_model, config.vocab_size)\n",
    "        \n",
    "        # Bias for each vocabulary token\n",
    "        self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n",
    "        \n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.dense(hidden_states)\n",
    "        x = F.gelu(x)\n",
    "        x = self.layer_norm(x)\n",
    "        logits = self.decoder(x) + self.bias\n",
    "        return logits\n",
    "\n",
    "# Test MLM head\n",
    "mlm_head = MLMHead(config)\n",
    "x = torch.randn(2, 20, config.d_model)\n",
    "out = mlm_head(x)\n",
    "assert out.shape == (2, 20, config.vocab_size)\n",
    "print(f\"MLM head test passed. Output shape: {out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Full BERT Model for MLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTForMLM(nn.Module):\n",
    "    \n",
    "    def __init__(self, config: BERTConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Embeddings\n",
    "        self.embeddings = BERTEmbeddings(config)\n",
    "        \n",
    "        # Encoder layers\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            BERTEncoderLayer(config) for _ in range(config.n_layers)\n",
    "        ])\n",
    "        \n",
    "        # MLM head\n",
    "        self.mlm_head = MLMHead(config)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "            \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        # Get embeddings\n",
    "        hidden_states = self.embeddings(input_ids, token_type_ids)\n",
    "        \n",
    "        # Apply encoder layers\n",
    "        for layer in self.encoder_layers:\n",
    "            hidden_states = layer(hidden_states, attention_mask)\n",
    "        \n",
    "        # Get MLM predictions\n",
    "        logits = self.mlm_head(hidden_states)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Create and test model\n",
    "model = BERTForMLM(config).to(device)\n",
    "\n",
    "# Test forward pass\n",
    "batch = next(iter(mlm_dataloader))\n",
    "input_ids = batch['input_ids'].to(device)\n",
    "attention_mask = batch['attention_mask'].to(device)\n",
    "token_type_ids = batch['token_type_ids'].to(device)\n",
    "\n",
    "logits = model(input_ids, attention_mask, token_type_ids)\n",
    "assert logits.shape == (config.batch_size, config.max_seq_len, config.vocab_size)\n",
    "print(f\"Model test passed. Output shape: {logits.shape}\")\n",
    "\n",
    "# Count parameters\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model parameters: {num_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check 1: Only masked positions contribute to loss\n",
    "print(\"Verifying loss computation...\")\n",
    "\n",
    "batch = next(iter(mlm_dataloader))\n",
    "input_ids = batch['input_ids'].to(device)\n",
    "attention_mask = batch['attention_mask'].to(device)\n",
    "token_type_ids = batch['token_type_ids'].to(device)\n",
    "labels = batch['labels'].to(device)\n",
    "\n",
    "# Forward pass\n",
    "logits = model(input_ids, attention_mask, token_type_ids)\n",
    "\n",
    "# Compute loss with ignore_index=-100\n",
    "loss = F.cross_entropy(\n",
    "    logits.view(-1, config.vocab_size),\n",
    "    labels.view(-1),\n",
    "    ignore_index=-100\n",
    ")\n",
    "\n",
    "# Verify masked positions only\n",
    "num_masked = (labels != -100).sum().item()\n",
    "print(f\"Number of masked positions: {num_masked}\")\n",
    "print(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Manually verify\n",
    "manual_loss = 0.0\n",
    "count = 0\n",
    "for i in range(labels.size(0)):\n",
    "    for j in range(labels.size(1)):\n",
    "        if labels[i, j] != -100:\n",
    "            manual_loss += F.cross_entropy(\n",
    "                logits[i, j].unsqueeze(0),\n",
    "                labels[i, j].unsqueeze(0)\n",
    "            ).item()\n",
    "            count += 1\n",
    "\n",
    "manual_loss /= count\n",
    "print(f\"Manual loss: {manual_loss:.4f}\")\n",
    "assert abs(loss.item() - manual_loss) < 0.01, \"Loss mismatch!\"\n",
    "print(\"\u2713 Loss is correctly computed only for masked positions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check 2: Gradient flow\n",
    "print(\"\\nVerifying gradient flow...\")\n",
    "\n",
    "model.zero_grad()\n",
    "loss.backward()\n",
    "\n",
    "# Check that all parameters have gradients\n",
    "grad_norms = []\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        assert param.grad is not None, f\"No gradient for {name}\"\n",
    "        assert not torch.isnan(param.grad).any(), f\"NaN gradient for {name}\"\n",
    "        grad_norms.append((name, param.grad.norm().item()))\n",
    "\n",
    "print(\"Sample gradient norms:\")\n",
    "for name, norm in grad_norms[:5]:\n",
    "    print(f\"  {name}: {norm:.6f}\")\n",
    "print(\"\u2713 All gradients are valid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mlm_accuracy(\n",
    "    logits: torch.Tensor,\n",
    "    labels: torch.Tensor\n",
    ") -> float:\n",
    "    predictions = logits.argmax(dim=-1)  # (batch, seq_len)\n",
    "    mask = labels != -100\n",
    "    \n",
    "    if mask.sum() == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    correct = (predictions == labels) & mask\n",
    "    accuracy = correct.sum().float() / mask.sum().float()\n",
    "    return accuracy.item()\n",
    "\n",
    "\n",
    "def train_bert_epoch(\n",
    "    model: BERTForMLM,\n",
    "    dataloader: DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    device: torch.device\n",
    ") -> Tuple[float, float]:\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_accuracy = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Training\", leave=False):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        token_type_ids = batch['token_type_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(input_ids, attention_mask, token_type_ids)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = F.cross_entropy(\n",
    "            logits.view(-1, model.config.vocab_size),\n",
    "            labels.view(-1),\n",
    "            ignore_index=-100\n",
    "        )\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        total_loss += loss.item()\n",
    "        total_accuracy += compute_mlm_accuracy(logits, labels)\n",
    "        num_batches += 1\n",
    "    \n",
    "    return total_loss / num_batches, total_accuracy / num_batches\n",
    "\n",
    "\n",
    "def evaluate_bert(\n",
    "    model: BERTForMLM,\n",
    "    dataloader: DataLoader,\n",
    "    device: torch.device\n",
    ") -> Tuple[float, float]:\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_accuracy = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            logits = model(input_ids, attention_mask, token_type_ids)\n",
    "            \n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(-1, model.config.vocab_size),\n",
    "                labels.view(-1),\n",
    "                ignore_index=-100\n",
    "            )\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_accuracy += compute_mlm_accuracy(logits, labels)\n",
    "            num_batches += 1\n",
    "    \n",
    "    return total_loss / num_batches, total_accuracy / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bert_model(\n",
    "    config: BERTConfig,\n",
    "    dataloader: DataLoader,\n",
    "    device: torch.device,\n",
    "    model_name: str = \"BERT\"\n",
    ") -> Tuple[BERTForMLM, List[float], List[float]]:\n",
    "    model = BERTForMLM(config).to(device)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config.learning_rate,\n",
    "        betas=(0.9, 0.999),\n",
    "        weight_decay=0.01\n",
    "    )\n",
    "    \n",
    "    # Learning rate scheduler with warmup\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=config.n_epochs\n",
    "    )\n",
    "    \n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training {model_name}\")\n",
    "    print(f\"Layers: {config.n_layers}, Mask prob: {config.mask_prob}\")\n",
    "    print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(config.n_epochs):\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        train_loss, train_acc = train_bert_epoch(model, dataloader, optimizer, device)\n",
    "        \n",
    "        # Step scheduler\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Record\n",
    "        losses.append(train_loss)\n",
    "        accuracies.append(train_acc)\n",
    "        \n",
    "        epoch_time = time.time() - epoch_start\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{config.n_epochs} | \"\n",
    "              f\"Loss: {train_loss:.4f} | \"\n",
    "              f\"MLM Acc: {train_acc:.2%} | \"\n",
    "              f\"Time: {epoch_time:.1f}s\")\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nTotal training time: {total_time:.1f}s\")\n",
    "    \n",
    "    return model, losses, accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Training with Default Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with default config (4 layers, 15% masking)\n",
    "set_seed(42)\n",
    "model_default, losses_default, acc_default = train_bert_model(\n",
    "    config, mlm_dataloader, device, \"Default (4 layers, 15% mask)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Ablation 1: Mask Probability (0.15 vs 0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset with 20% masking\n",
    "config_20mask = BERTConfig(\n",
    "    d_model=config.d_model,\n",
    "    n_heads=config.n_heads,\n",
    "    n_layers=config.n_layers,\n",
    "    d_ff=config.d_ff,\n",
    "    max_seq_len=config.max_seq_len,\n",
    "    vocab_size=config.vocab_size,\n",
    "    dropout=config.dropout,\n",
    "    mask_prob=0.20,  # Increased from 0.15\n",
    "    batch_size=config.batch_size,\n",
    "    learning_rate=config.learning_rate,\n",
    "    n_epochs=config.n_epochs\n",
    ")\n",
    "\n",
    "# Create new dataset with 20% masking\n",
    "mlm_dataset_20 = MLMDataset(TEXTS, tokenizer, config.max_seq_len, mask_prob=0.20)\n",
    "mlm_dataloader_20 = DataLoader(mlm_dataset_20, batch_size=config.batch_size, shuffle=True)\n",
    "\n",
    "set_seed(42)\n",
    "model_20mask, losses_20mask, acc_20mask = train_bert_model(\n",
    "    config_20mask, mlm_dataloader_20, device, \"20% Masking\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Ablation 2: Number of Layers (2 vs 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create config with 2 layers\n",
    "config_2layers = BERTConfig(\n",
    "    d_model=config.d_model,\n",
    "    n_heads=config.n_heads,\n",
    "    n_layers=2,  # Reduced from 4\n",
    "    d_ff=config.d_ff,\n",
    "    max_seq_len=config.max_seq_len,\n",
    "    vocab_size=config.vocab_size,\n",
    "    dropout=config.dropout,\n",
    "    mask_prob=config.mask_prob,\n",
    "    batch_size=config.batch_size,\n",
    "    learning_rate=config.learning_rate,\n",
    "    n_epochs=config.n_epochs\n",
    ")\n",
    "\n",
    "set_seed(42)\n",
    "model_2layers, losses_2layers, acc_2layers = train_bert_model(\n",
    "    config_2layers, mlm_dataloader, device, \"2 Layers\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss curves\n",
    "axes[0].plot(losses_default, label='4 layers, 15% mask', marker='o')\n",
    "axes[0].plot(losses_20mask, label='4 layers, 20% mask', marker='s')\n",
    "axes[0].plot(losses_2layers, label='2 layers, 15% mask', marker='^')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Training Loss')\n",
    "axes[0].set_title('MLM Training Loss Comparison')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy curves\n",
    "axes[1].plot(acc_default, label='4 layers, 15% mask', marker='o')\n",
    "axes[1].plot(acc_20mask, label='4 layers, 20% mask', marker='s')\n",
    "axes[1].plot(acc_2layers, label='2 layers, 15% mask', marker='^')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('MLM Accuracy')\n",
    "axes[1].set_title('MLM Accuracy Comparison')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: '{:.0%}'.format(y)))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Configuration':<30} {'Final Loss':<15} {'Final MLM Acc':<15} {'Params':<15}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "configs_results = [\n",
    "    (\"4 layers, 15% mask (default)\", losses_default[-1], acc_default[-1], \n",
    "     sum(p.numel() for p in model_default.parameters())),\n",
    "    (\"4 layers, 20% mask\", losses_20mask[-1], acc_20mask[-1],\n",
    "     sum(p.numel() for p in model_20mask.parameters())),\n",
    "    (\"2 layers, 15% mask\", losses_2layers[-1], acc_2layers[-1],\n",
    "     sum(p.numel() for p in model_2layers.parameters())),\n",
    "]\n",
    "\n",
    "for name, loss, acc, params in configs_results:\n",
    "    print(f\"{name:<30} {loss:<15.4f} {acc:<15.2%} {params:,}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. MLM Inference Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_masked_tokens(\n",
    "    model: BERTForMLM,\n",
    "    text: str,\n",
    "    tokenizer: BERTTokenizer,\n",
    "    device: torch.device,\n",
    "    top_k: int = 5\n",
    ") -> None:\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize (but keep [MASK] tokens)\n",
    "    words = text.lower().split()\n",
    "    token_ids = []\n",
    "    mask_positions = []\n",
    "    \n",
    "    token_ids.append(tokenizer.cls_idx)\n",
    "    for i, word in enumerate(words):\n",
    "        if word == '[mask]':\n",
    "            token_ids.append(tokenizer.mask_idx)\n",
    "            mask_positions.append(len(token_ids) - 1)\n",
    "        else:\n",
    "            idx = tokenizer.word2idx.get(word, tokenizer.unk_idx)\n",
    "            token_ids.append(idx)\n",
    "    token_ids.append(tokenizer.sep_idx)\n",
    "    \n",
    "    # Create tensors\n",
    "    input_ids = torch.tensor([token_ids], dtype=torch.long).to(device)\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    \n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids, attention_mask)\n",
    "    \n",
    "    print(f\"Input: {text}\")\n",
    "    print(f\"\\nPredictions:\")\n",
    "    \n",
    "    for pos in mask_positions:\n",
    "        probs = F.softmax(logits[0, pos], dim=-1)\n",
    "        top_probs, top_indices = probs.topk(top_k)\n",
    "        \n",
    "        print(f\"\\nPosition {pos}:\")\n",
    "        for prob, idx in zip(top_probs, top_indices):\n",
    "            word = tokenizer.idx2word.get(idx.item(), '[UNK]')\n",
    "            print(f\"  {word}: {prob.item():.4f}\")\n",
    "\n",
    "# Test prediction\n",
    "print(\"=\" * 60)\n",
    "print(\"MLM INFERENCE DEMO\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "predict_masked_tokens(\n",
    "    model_default,\n",
    "    \"The quick brown [MASK] jumps over the lazy dog\",\n",
    "    tokenizer,\n",
    "    device\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "\n",
    "predict_masked_tokens(\n",
    "    model_default,\n",
    "    \"The [MASK] rose slowly over the mountains\",\n",
    "    tokenizer,\n",
    "    device\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}