{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Transformer from Scratch\n",
    "\n",
    "Deep learning paper implementation from scratch using PyTorch.\n",
    "1. **Sinusoidal Positional Encoding** - Fixed position embeddings using sine/cosine functions\n",
    "- Positional encoding ON vs OFF\n",
    "- Number of attention heads: 4 vs 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import time\n",
    "from typing import Optional, Tuple, List, Dict\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 42):\n",
    "    # Set all random seeds for reproducibility.\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "set_seed(42)\n",
    "\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TransformerConfig:\n",
    "\n",
    "    # Model architecture\n",
    "    d_model: int = 256          # Embedding dimension\n",
    "    n_heads: int = 8            # Number of attention heads\n",
    "    n_layers: int = 4           # Number of encoder/decoder layers\n",
    "    d_ff: int = 512             # Feed-forward hidden dimension\n",
    "    max_seq_len: int = 128      # Maximum sequence length\n",
    "    vocab_size: int = 10000     # Vocabulary size (will be updated)\n",
    "    dropout: float = 0.1        # Dropout rate\n",
    "    \n",
    "    # Training\n",
    "    batch_size: int = 32\n",
    "    learning_rate: float = 1e-4\n",
    "    n_epochs: int = 10\n",
    "    \n",
    "    # Ablation flags\n",
    "    use_positional_encoding: bool = True\n",
    "    \n",
    "config = TransformerConfig()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In practice, you would load WikiText-2 or another corpus\n",
    "SAMPLE_CORPUS = \"\"\"\n",
    "The quick brown fox jumps over the lazy dog.\n",
    "A journey of a thousand miles begins with a single step.\n",
    "To be or not to be that is the question.\n",
    "All that glitters is not gold.\n",
    "The only thing we have to fear is fear itself.\n",
    "In the beginning was the word and the word was with god.\n",
    "It was the best of times it was the worst of times.\n",
    "Call me ishmael some years ago never mind how long precisely.\n",
    "It is a truth universally acknowledged that a single man in possession of a good fortune must be in want of a wife.\n",
    "Happy families are all alike every unhappy family is unhappy in its own way.\n",
    "The sun rose slowly over the mountains casting long shadows across the valley below.\n",
    "She walked through the forest listening to the birds singing their morning songs.\n",
    "The old man sat by the fire remembering the days of his youth.\n",
    "The city streets were busy with people hurrying to their destinations.\n",
    "A gentle breeze blew through the open window bringing the scent of flowers.\n",
    "The children played in the garden while their parents watched from the porch.\n",
    "He picked up the book and began to read losing himself in the story.\n",
    "The stars twinkled in the night sky like diamonds scattered across velvet.\n",
    "She smiled at the memory of their first meeting so many years ago.\n",
    "The waves crashed against the shore creating a soothing rhythm.\n",
    "Deep in the forest there lived a wise old owl who knew many secrets.\n",
    "The train departed from the station carrying passengers to distant lands.\n",
    "Music filled the air as the orchestra began their evening performance.\n",
    "The scientist worked late into the night trying to solve the puzzle.\n",
    "Rain began to fall gently at first then harder until it became a downpour.\n",
    "\"\"\"\n",
    "\n",
    "# Repeat to create more data\n",
    "CORPUS = (SAMPLE_CORPUS * 50).lower()\n",
    "print(f\"Corpus length: {len(CORPUS)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "    \n",
    "    def __init__(self, min_freq: int = 1):\n",
    "        self.min_freq = min_freq\n",
    "        self.word2idx: Dict[str, int] = {}\n",
    "        self.idx2word: Dict[int, str] = {}\n",
    "        \n",
    "        # Special tokens\n",
    "        self.pad_token = \"<PAD>\"\n",
    "        self.unk_token = \"<UNK>\"\n",
    "        self.bos_token = \"<BOS>\"\n",
    "        self.eos_token = \"<EOS>\"\n",
    "        self.mask_token = \"<MASK>\"\n",
    "        \n",
    "        self.special_tokens = [\n",
    "            self.pad_token, self.unk_token, \n",
    "            self.bos_token, self.eos_token, \n",
    "            self.mask_token\n",
    "        ]\n",
    "        \n",
    "    def fit(self, text: str) -> None:\n",
    "        # Count word frequencies\n",
    "        word_freq: Dict[str, int] = {}\n",
    "        words = text.split()\n",
    "        for word in words:\n",
    "            word_freq[word] = word_freq.get(word, 0) + 1\n",
    "        \n",
    "        # Build vocabulary\n",
    "        for i, token in enumerate(self.special_tokens):\n",
    "            self.word2idx[token] = i\n",
    "            self.idx2word[i] = token\n",
    "        \n",
    "        idx = len(self.special_tokens)\n",
    "        for word, freq in sorted(word_freq.items()):\n",
    "            if freq >= self.min_freq:\n",
    "                self.word2idx[word] = idx\n",
    "                self.idx2word[idx] = word\n",
    "                idx += 1\n",
    "                \n",
    "        print(f\"Vocabulary size: {len(self.word2idx)}\")\n",
    "    \n",
    "    def encode(self, text: str) -> List[int]:\n",
    "\n",
    "        unk_idx = self.word2idx[self.unk_token]\n",
    "        return [self.word2idx.get(word, unk_idx) for word in text.split()]\n",
    "    \n",
    "    def decode(self, indices: List[int]) -> str:\n",
    "\n",
    "        return \" \".join([self.idx2word.get(idx, self.unk_token) for idx in indices])\n",
    "    \n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        return len(self.word2idx)\n",
    "    \n",
    "    @property\n",
    "    def pad_idx(self) -> int:\n",
    "        return self.word2idx[self.pad_token]\n",
    "\n",
    "# Build tokenizer\n",
    "tokenizer = SimpleTokenizer(min_freq=2)\n",
    "tokenizer.fit(CORPUS)\n",
    "\n",
    "# Update config\n",
    "config.vocab_size = tokenizer.vocab_size\n",
    "print(f\"Updated vocab size: {config.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModelingDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, text: str, tokenizer: SimpleTokenizer, seq_len: int):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        # Tokenize entire corpus\n",
    "        self.tokens = tokenizer.encode(text)\n",
    "        print(f\"Total tokens: {len(self.tokens)}\")\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        # Number of sequences we can create\n",
    "        return max(0, len(self.tokens) - self.seq_len)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # Input: tokens[idx:idx+seq_len]\n",
    "        # Target: tokens[idx+1:idx+seq_len+1] (shifted by 1)\n",
    "        x = torch.tensor(self.tokens[idx:idx + self.seq_len], dtype=torch.long)\n",
    "        y = torch.tensor(self.tokens[idx + 1:idx + self.seq_len + 1], dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = LanguageModelingDataset(CORPUS, tokenizer, config.max_seq_len)\n",
    "dataloader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=config.batch_size, \n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "print(f\"Number of batches: {len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Transformer Components\n",
    "\n",
    "Now we build each component of the Transformer step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Sinusoidal Positional Encoding\n",
    "\n",
    "The positional encoding adds position information to embeddings using sine and cosine functions:\n",
    "\n",
    "$$PE_{(pos, 2i)} = \\sin(pos / 10000^{2i/d_{model}})$$\n",
    "$$PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i/d_{model}})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPositionalEncoding(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model: int, max_seq_len: int = 5000, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Create positional encoding matrix\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        # Compute the div_term: 10000^(2i/d_model)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        pe = pe.unsqueeze(0)  # Shape: (1, max_seq_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        seq_len = x.size(1)\n",
    "        x = x + self.pe[:, :seq_len, :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# Test positional encoding\n",
    "pe = SinusoidalPositionalEncoding(d_model=config.d_model, max_seq_len=config.max_seq_len)\n",
    "test_input = torch.zeros(2, 10, config.d_model)\n",
    "test_output = pe(test_input)\n",
    "assert test_output.shape == test_input.shape, \"Shape mismatch!\"\n",
    "print(f\"Positional encoding test passed. Output shape: {test_output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize positional encoding\n",
    "pe_viz = SinusoidalPositionalEncoding(d_model=64, max_seq_len=100)\n",
    "pe_matrix = pe_viz.pe[0, :50, :].numpy()\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.imshow(pe_matrix.T, aspect='auto', cmap='RdBu')\n",
    "plt.xlabel('Position')\n",
    "plt.ylabel('Dimension')\n",
    "plt.title('Sinusoidal Positional Encoding')\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Scaled Dot-Product Attention\n",
    "\n",
    "The attention mechanism computes:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "The scaling factor $\\sqrt{d_k}$ prevents the dot products from becoming too large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(\n",
    "    query: torch.Tensor,\n",
    "    key: torch.Tensor,\n",
    "    value: torch.Tensor,\n",
    "    mask: Optional[torch.Tensor] = None,\n",
    "    dropout: Optional[nn.Dropout] = None\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "\n",
    "    d_k = query.size(-1)\n",
    "    \n",
    "    # Compute attention scores: Q @ K^T / sqrt(d_k)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    \n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "    \n",
    "    # Softmax to get attention weights\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    # Apply dropout if provided\n",
    "    if dropout is not None:\n",
    "        attention_weights = dropout(attention_weights)\n",
    "    \n",
    "    # Compute output: attention_weights @ V\n",
    "    output = torch.matmul(attention_weights, value)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "# Test scaled dot-product attention\n",
    "batch, n_heads, seq_len, d_k = 2, 4, 10, 32\n",
    "q = torch.randn(batch, n_heads, seq_len, d_k)\n",
    "k = torch.randn(batch, n_heads, seq_len, d_k)\n",
    "v = torch.randn(batch, n_heads, seq_len, d_k)\n",
    "\n",
    "out, attn_weights = scaled_dot_product_attention(q, k, v)\n",
    "assert out.shape == (batch, n_heads, seq_len, d_k), f\"Output shape mismatch: {out.shape}\"\n",
    "assert attn_weights.shape == (batch, n_heads, seq_len, seq_len), f\"Attention weights shape mismatch: {attn_weights.shape}\"\n",
    "assert torch.allclose(attn_weights.sum(dim=-1), torch.ones(batch, n_heads, seq_len)), \"Attention weights don't sum to 1!\"\n",
    "print(f\"Scaled dot-product attention test passed.\")\n",
    "print(f\"Output shape: {out.shape}, Attention weights shape: {attn_weights.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Multi-Head Attention\n",
    "\n",
    "Multi-head attention allows the model to attend to information from different representation subspaces:\n",
    "\n",
    "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1, ..., head_h)W^O$$\n",
    "\n",
    "where $head_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model: int, n_heads: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads  # Dimension per head\n",
    "        \n",
    "        # Linear projections for Q, K, V\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Output projection\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Store attention weights for visualization\n",
    "        self.attention_weights = None\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        query: torch.Tensor,\n",
    "        key: torch.Tensor,\n",
    "        value: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        # Linear projections\n",
    "        Q = self.W_q(query)  # (batch, seq_len, d_model)\n",
    "        K = self.W_k(key)\n",
    "        V = self.W_v(value)\n",
    "        \n",
    "        # Reshape to (batch, n_heads, seq_len, d_k)\n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Apply scaled dot-product attention\n",
    "        attn_output, self.attention_weights = scaled_dot_product_attention(\n",
    "            Q, K, V, mask=mask, dropout=self.dropout\n",
    "        )\n",
    "        \n",
    "        # Reshape back: (batch, seq_len, d_model)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(\n",
    "            batch_size, -1, self.d_model\n",
    "        )\n",
    "        \n",
    "        # Final projection\n",
    "        output = self.W_o(attn_output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Test Multi-Head Attention\n",
    "mha = MultiHeadAttention(d_model=config.d_model, n_heads=config.n_heads)\n",
    "x = torch.randn(2, 10, config.d_model)\n",
    "out = mha(x, x, x)  # Self-attention\n",
    "assert out.shape == x.shape, f\"MHA output shape mismatch: {out.shape}\"\n",
    "print(f\"Multi-Head Attention test passed. Output shape: {out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Position-wise Feed-Forward Network\n",
    "\n",
    "$$\\text{FFN}(x) = \\text{ReLU}(xW_1 + b_1)W_2 + b_2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "\n",
    "    \n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.linear2(self.dropout(F.relu(self.linear1(x))))\n",
    "\n",
    "# Test FFN\n",
    "ffn = PositionwiseFeedForward(config.d_model, config.d_ff)\n",
    "x = torch.randn(2, 10, config.d_model)\n",
    "out = ffn(x)\n",
    "assert out.shape == x.shape, f\"FFN output shape mismatch: {out.shape}\"\n",
    "print(f\"FFN test passed. Output shape: {out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Encoder Layer\n",
    "\n",
    "Each encoder layer consists of:\n",
    "1. Multi-head self-attention\n",
    "2. Add & Norm (residual connection + layer normalization)\n",
    "3. Feed-forward network\n",
    "4. Add & Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.self_attention = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "   \n",
    "        # Self-attention with residual connection and layer norm\n",
    "        attn_output = self.self_attention(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        # Feed-forward with residual connection and layer norm\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Test encoder layer\n",
    "enc_layer = EncoderLayer(config.d_model, config.n_heads, config.d_ff)\n",
    "x = torch.randn(2, 10, config.d_model)\n",
    "out = enc_layer(x)\n",
    "assert out.shape == x.shape\n",
    "print(f\"Encoder layer test passed. Output shape: {out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6 Decoder Layer\n",
    "\n",
    "Each decoder layer has:\n",
    "1. Masked multi-head self-attention (causal)\n",
    "2. Add & Norm\n",
    "3. Multi-head cross-attention (attending to encoder output)\n",
    "4. Add & Norm\n",
    "5. Feed-forward network\n",
    "6. Add & Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "\n",
    "    \n",
    "    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.self_attention = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.cross_attention = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        encoder_output: torch.Tensor,\n",
    "        self_mask: Optional[torch.Tensor] = None,\n",
    "        cross_mask: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "   \n",
    "        # Masked self-attention\n",
    "        attn_output = self.self_attention(x, x, x, self_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        # Cross-attention (attend to encoder output)\n",
    "        cross_output = self.cross_attention(x, encoder_output, encoder_output, cross_mask)\n",
    "        x = self.norm2(x + self.dropout(cross_output))\n",
    "        \n",
    "        # Feed-forward\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Test decoder layer\n",
    "dec_layer = DecoderLayer(config.d_model, config.n_heads, config.d_ff)\n",
    "x = torch.randn(2, 10, config.d_model)\n",
    "enc_out = torch.randn(2, 15, config.d_model)\n",
    "out = dec_layer(x, enc_out)\n",
    "assert out.shape == x.shape\n",
    "print(f\"Decoder layer test passed. Output shape: {out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.7 Mask Generation\n",
    "\n",
    "We need two types of masks:\n",
    "1. **Padding mask**: Prevents attention to padding tokens\n",
    "2. **Causal mask**: Prevents attention to future tokens (for decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq: torch.Tensor, pad_idx: int) -> torch.Tensor:\n",
    "    return (seq != pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "\n",
    "def create_causal_mask(size: int, device: torch.device) -> torch.Tensor:\n",
    "    mask = torch.tril(torch.ones(size, size, device=device)).bool()\n",
    "    return mask.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "\n",
    "# Test masks\n",
    "test_seq = torch.tensor([[1, 2, 3, 0, 0], [1, 2, 0, 0, 0]])\n",
    "pad_mask = create_padding_mask(test_seq, pad_idx=0)\n",
    "print(f\"Padding mask shape: {pad_mask.shape}\")\n",
    "print(f\"Padding mask example:\\n{pad_mask[0, 0, 0]}\")\n",
    "\n",
    "causal_mask = create_causal_mask(5, device='cpu')\n",
    "print(f\"\\nCausal mask shape: {causal_mask.shape}\")\n",
    "print(f\"Causal mask:\\n{causal_mask[0, 0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check: masking correctness\n",
    "# Verify that causal mask prevents attending to future positions\n",
    "\n",
    "seq_len = 5\n",
    "causal = create_causal_mask(seq_len, 'cpu')\n",
    "\n",
    "for i in range(seq_len):\n",
    "    allowed_positions = causal[0, 0, i, :].sum().item()\n",
    "    assert allowed_positions == i + 1, f\"Position {i} should attend to {i+1} positions, got {allowed_positions}\"\n",
    "\n",
    "print(\"Causal mask correctness verified!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Full Transformer Model (Encoder-Decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderDecoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Embeddings\n",
    "        self.src_embedding = nn.Embedding(config.vocab_size, config.d_model)\n",
    "        self.tgt_embedding = nn.Embedding(config.vocab_size, config.d_model)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.positional_encoding = SinusoidalPositionalEncoding(\n",
    "            config.d_model, config.max_seq_len, config.dropout\n",
    "        )\n",
    "        \n",
    "        # Encoder layers\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            EncoderLayer(config.d_model, config.n_heads, config.d_ff, config.dropout)\n",
    "            for _ in range(config.n_layers)\n",
    "        ])\n",
    "        \n",
    "        # Decoder layers\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            DecoderLayer(config.d_model, config.n_heads, config.d_ff, config.dropout)\n",
    "            for _ in range(config.n_layers)\n",
    "        ])\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_projection = nn.Linear(config.d_model, config.vocab_size)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "                \n",
    "    def encode(self, src: torch.Tensor, src_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        x = self.src_embedding(src) * math.sqrt(self.config.d_model)\n",
    "        if self.config.use_positional_encoding:\n",
    "            x = self.positional_encoding(x)\n",
    "        \n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x, src_mask)\n",
    "        return x\n",
    "    \n",
    "    def decode(\n",
    "        self,\n",
    "        tgt: torch.Tensor,\n",
    "        encoder_output: torch.Tensor,\n",
    "        tgt_mask: Optional[torch.Tensor] = None,\n",
    "        src_mask: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        x = self.tgt_embedding(tgt) * math.sqrt(self.config.d_model)\n",
    "        if self.config.use_positional_encoding:\n",
    "            x = self.positional_encoding(x)\n",
    "        \n",
    "        for layer in self.decoder_layers:\n",
    "            x = layer(x, encoder_output, tgt_mask, src_mask)\n",
    "        return x\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        src: torch.Tensor,\n",
    "        tgt: torch.Tensor,\n",
    "        src_mask: Optional[torch.Tensor] = None,\n",
    "        tgt_mask: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        encoder_output = self.encode(src, src_mask)\n",
    "        decoder_output = self.decode(tgt, encoder_output, tgt_mask, src_mask)\n",
    "        logits = self.output_projection(decoder_output)\n",
    "        return logits\n",
    "\n",
    "# Test encoder-decoder\n",
    "enc_dec_model = TransformerEncoderDecoder(config)\n",
    "src = torch.randint(0, config.vocab_size, (2, 10))\n",
    "tgt = torch.randint(0, config.vocab_size, (2, 8))\n",
    "output = enc_dec_model(src, tgt)\n",
    "assert output.shape == (2, 8, config.vocab_size)\n",
    "print(f\"Encoder-Decoder test passed. Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Decoder-Only Transformer (for Language Modeling)\n",
    "\n",
    "For causal language modeling, we only need the decoder part with causal masking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderOnlyTransformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Token embedding\n",
    "        self.token_embedding = nn.Embedding(config.vocab_size, config.d_model)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.positional_encoding = SinusoidalPositionalEncoding(\n",
    "            config.d_model, config.max_seq_len, config.dropout\n",
    "        )\n",
    "        \n",
    "        # Transformer layers (decoder blocks without cross-attention)\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(config.d_model, config.n_heads, config.d_ff, config.dropout)\n",
    "            for _ in range(config.n_layers)\n",
    "        ])\n",
    "        \n",
    "        # Final layer norm\n",
    "        self.norm = nn.LayerNorm(config.d_model)\n",
    "        \n",
    "        # Output projection (tied with embeddings for efficiency)\n",
    "        self.output_projection = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
    "        \n",
    "        # Weight tying\n",
    "        self.output_projection.weight = self.token_embedding.weight\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "        \n",
    "        # Store the causal mask\n",
    "        self.register_buffer(\n",
    "            'causal_mask',\n",
    "            create_causal_mask(config.max_seq_len, 'cpu')[0, 0]\n",
    "        )\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "                if module.bias is not None:\n",
    "                    torch.nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "                \n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        pad_mask: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        batch_size, seq_len = x.shape\n",
    "        \n",
    "        # Create causal mask\n",
    "        causal_mask = self.causal_mask[:seq_len, :seq_len].unsqueeze(0).unsqueeze(0)\n",
    "        causal_mask = causal_mask.expand(batch_size, 1, seq_len, seq_len)\n",
    "        \n",
    "        # Combine with padding mask if provided\n",
    "        if pad_mask is not None:\n",
    "            mask = causal_mask & pad_mask\n",
    "        else:\n",
    "            mask = causal_mask\n",
    "            \n",
    "        # Embed tokens\n",
    "        h = self.token_embedding(x) * math.sqrt(self.config.d_model)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        if self.config.use_positional_encoding:\n",
    "            h = self.positional_encoding(h)\n",
    "        \n",
    "        # Apply transformer layers\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, mask)\n",
    "            \n",
    "        # Final normalization and projection\n",
    "        h = self.norm(h)\n",
    "        logits = self.output_projection(h)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def generate(\n",
    "        self,\n",
    "        prompt: torch.Tensor,\n",
    "        max_new_tokens: int = 50,\n",
    "        temperature: float = 1.0\n",
    "    ) -> torch.Tensor:\n",
    "        self.eval()\n",
    "        x = prompt.clone()\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "            # Truncate if too long\n",
    "            x_cond = x if x.size(1) <= self.config.max_seq_len else x[:, -self.config.max_seq_len:]\n",
    "            \n",
    "            # Get predictions\n",
    "            with torch.no_grad():\n",
    "                logits = self(x_cond)\n",
    "                logits = logits[:, -1, :] / temperature\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "                x = torch.cat([x, next_token], dim=1)\n",
    "                \n",
    "        return x\n",
    "\n",
    "# Test decoder-only model\n",
    "model = DecoderOnlyTransformer(config)\n",
    "x = torch.randint(0, config.vocab_size, (2, 20))\n",
    "output = model(x)\n",
    "assert output.shape == (2, 20, config.vocab_size)\n",
    "print(f\"Decoder-only test passed. Output shape: {output.shape}\")\n",
    "\n",
    "# Count parameters\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model parameters: {num_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    device: torch.device,\n",
    "    pad_idx: int\n",
    ") -> Tuple[float, int]:\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    for batch_x, batch_y in tqdm(dataloader, desc=\"Training\", leave=False):\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(batch_x)\n",
    "        \n",
    "        # Compute loss (ignore padding)\n",
    "        loss = F.cross_entropy(\n",
    "            logits.view(-1, logits.size(-1)),\n",
    "            batch_y.view(-1),\n",
    "            ignore_index=pad_idx\n",
    "        )\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track stats\n",
    "        num_tokens = (batch_y != pad_idx).sum().item()\n",
    "        total_loss += loss.item() * num_tokens\n",
    "        total_tokens += num_tokens\n",
    "        \n",
    "    return total_loss / total_tokens, total_tokens\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    device: torch.device,\n",
    "    pad_idx: int\n",
    ") -> float:\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in dataloader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            \n",
    "            logits = model(batch_x)\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(-1, logits.size(-1)),\n",
    "                batch_y.view(-1),\n",
    "                ignore_index=pad_idx,\n",
    "                reduction='sum'\n",
    "            )\n",
    "            \n",
    "            num_tokens = (batch_y != pad_idx).sum().item()\n",
    "            total_loss += loss.item()\n",
    "            total_tokens += num_tokens\n",
    "            \n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    config: TransformerConfig,\n",
    "    dataloader: DataLoader,\n",
    "    device: torch.device,\n",
    "    tokenizer: SimpleTokenizer,\n",
    "    model_name: str = \"Transformer\"\n",
    ") -> Tuple[DecoderOnlyTransformer, List[float], List[float]]:\n",
    "    model = DecoderOnlyTransformer(config).to(device)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config.learning_rate,\n",
    "        betas=(0.9, 0.98),\n",
    "        eps=1e-9\n",
    "    )\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=config.n_epochs\n",
    "    )\n",
    "    \n",
    "    train_losses = []\n",
    "    perplexities = []\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training {model_name}\")\n",
    "    print(f\"Config: heads={config.n_heads}, pos_enc={config.use_positional_encoding}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    total_tokens_processed = 0\n",
    "    \n",
    "    for epoch in range(config.n_epochs):\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        train_loss, tokens = train_epoch(\n",
    "            model, dataloader, optimizer, device, tokenizer.pad_idx\n",
    "        )\n",
    "        total_tokens_processed += tokens\n",
    "        \n",
    "        perplexity = evaluate(model, dataloader, device, tokenizer.pad_idx)\n",
    "        \n",
    "        # Step scheduler\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Record\n",
    "        train_losses.append(train_loss)\n",
    "        perplexities.append(perplexity)\n",
    "        \n",
    "        epoch_time = time.time() - epoch_start\n",
    "        tokens_per_sec = tokens / epoch_time\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{config.n_epochs} | \"\n",
    "              f\"Loss: {train_loss:.4f} | \"\n",
    "              f\"PPL: {perplexity:.2f} | \"\n",
    "              f\"Time: {epoch_time:.1f}s | \"\n",
    "              f\"Tokens/sec: {tokens_per_sec:.0f}\")\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nTotal training time: {total_time:.1f}s\")\n",
    "    print(f\"Average tokens/sec: {total_tokens_processed / total_time:.0f}\")\n",
    "    \n",
    "    return model, train_losses, perplexities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check: Gradient flow\n",
    "print(\"Checking gradient flow...\")\n",
    "\n",
    "test_model = DecoderOnlyTransformer(config).to(device)\n",
    "test_x = torch.randint(0, config.vocab_size, (2, 20)).to(device)\n",
    "test_y = torch.randint(0, config.vocab_size, (2, 20)).to(device)\n",
    "\n",
    "test_model.zero_grad()\n",
    "logits = test_model(test_x)\n",
    "loss = F.cross_entropy(logits.view(-1, logits.size(-1)), test_y.view(-1))\n",
    "loss.backward()\n",
    "\n",
    "# Check that all parameters have gradients\n",
    "for name, param in test_model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        assert param.grad is not None, f\"No gradient for {name}\"\n",
    "        assert not torch.isnan(param.grad).any(), f\"NaN gradient for {name}\"\n",
    "        assert not torch.isinf(param.grad).any(), f\"Inf gradient for {name}\"\n",
    "        \n",
    "print(\"All gradients are valid!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check: Attention weights shape\n",
    "print(\"\\nChecking attention weights...\")\n",
    "\n",
    "mha = MultiHeadAttention(d_model=config.d_model, n_heads=config.n_heads)\n",
    "x = torch.randn(2, 15, config.d_model)  # batch=2, seq_len=15\n",
    "_ = mha(x, x, x)\n",
    "\n",
    "attn_weights = mha.attention_weights\n",
    "expected_shape = (2, config.n_heads, 15, 15)\n",
    "assert attn_weights.shape == expected_shape, f\"Expected {expected_shape}, got {attn_weights.shape}\"\n",
    "print(f\"Attention weights shape: {attn_weights.shape} ✓\")\n",
    "\n",
    "# Check attention weights sum to 1\n",
    "attn_sum = attn_weights.sum(dim=-1)\n",
    "assert torch.allclose(attn_sum, torch.ones_like(attn_sum), atol=1e-5), \"Attention weights don't sum to 1\"\n",
    "print(\"Attention weights sum to 1 ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Training with Default Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with default config (8 heads, with positional encoding)\n",
    "set_seed(42)\n",
    "model_default, losses_default, ppl_default = train_model(\n",
    "    config, dataloader, device, tokenizer, \"Default (8 heads, with PE)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Ablation 1: Positional Encoding ON vs OFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ablation: Without positional encoding\n",
    "config_no_pe = TransformerConfig(\n",
    "    d_model=config.d_model,\n",
    "    n_heads=config.n_heads,\n",
    "    n_layers=config.n_layers,\n",
    "    d_ff=config.d_ff,\n",
    "    max_seq_len=config.max_seq_len,\n",
    "    vocab_size=config.vocab_size,\n",
    "    dropout=config.dropout,\n",
    "    batch_size=config.batch_size,\n",
    "    learning_rate=config.learning_rate,\n",
    "    n_epochs=config.n_epochs,\n",
    "    use_positional_encoding=False  # Disable PE\n",
    ")\n",
    "\n",
    "set_seed(42)\n",
    "model_no_pe, losses_no_pe, ppl_no_pe = train_model(\n",
    "    config_no_pe, dataloader, device, tokenizer, \"Without Positional Encoding\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Ablation 2: Number of Heads (4 vs 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ablation: 4 attention heads\n",
    "config_4heads = TransformerConfig(\n",
    "    d_model=config.d_model,\n",
    "    n_heads=4,  # Changed from 8 to 4\n",
    "    n_layers=config.n_layers,\n",
    "    d_ff=config.d_ff,\n",
    "    max_seq_len=config.max_seq_len,\n",
    "    vocab_size=config.vocab_size,\n",
    "    dropout=config.dropout,\n",
    "    batch_size=config.batch_size,\n",
    "    learning_rate=config.learning_rate,\n",
    "    n_epochs=config.n_epochs,\n",
    "    use_positional_encoding=True\n",
    ")\n",
    "\n",
    "set_seed(42)\n",
    "model_4heads, losses_4heads, ppl_4heads = train_model(\n",
    "    config_4heads, dataloader, device, tokenizer, \"4 Attention Heads\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss curves\n",
    "axes[0].plot(losses_default, label='8 heads, with PE', marker='o')\n",
    "axes[0].plot(losses_no_pe, label='8 heads, no PE', marker='s')\n",
    "axes[0].plot(losses_4heads, label='4 heads, with PE', marker='^')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Training Loss')\n",
    "axes[0].set_title('Training Loss Comparison')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Perplexity curves\n",
    "axes[1].plot(ppl_default, label='8 heads, with PE', marker='o')\n",
    "axes[1].plot(ppl_no_pe, label='8 heads, no PE', marker='s')\n",
    "axes[1].plot(ppl_4heads, label='4 heads, with PE', marker='^')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Perplexity')\n",
    "axes[1].set_title('Perplexity Comparison')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results table\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Configuration':<30} {'Final Loss':<15} {'Final PPL':<15}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'8 heads, with PE (default)':<30} {losses_default[-1]:<15.4f} {ppl_default[-1]:<15.2f}\")\n",
    "print(f\"{'8 heads, without PE':<30} {losses_no_pe[-1]:<15.4f} {ppl_no_pe[-1]:<15.2f}\")\n",
    "print(f\"{'4 heads, with PE':<30} {losses_4heads[-1]:<15.4f} {ppl_4heads[-1]:<15.2f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Text Generation Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some text\n",
    "model_default.eval()\n",
    "\n",
    "# Create a prompt\n",
    "prompt_text = \"the quick brown\"\n",
    "prompt_tokens = tokenizer.encode(prompt_text)\n",
    "prompt_tensor = torch.tensor([prompt_tokens]).to(device)\n",
    "\n",
    "print(f\"Prompt: '{prompt_text}'\")\n",
    "print(\"\\nGenerated continuations:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for temp in [0.7, 1.0, 1.3]:\n",
    "    generated = model_default.generate(\n",
    "        prompt_tensor, \n",
    "        max_new_tokens=20,\n",
    "        temperature=temp\n",
    "    )\n",
    "    generated_text = tokenizer.decode(generated[0].tolist())\n",
    "    print(f\"Temperature {temp}: {generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Speed Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure tokens per second\n",
    "def measure_throughput(model, batch_size=32, seq_len=128, n_iters=50):\n",
    "    model.eval()\n",
    "    x = torch.randint(0, config.vocab_size, (batch_size, seq_len)).to(device)\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(5):\n",
    "        with torch.no_grad():\n",
    "            _ = model(x)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    start = time.time()\n",
    "    for _ in range(n_iters):\n",
    "        with torch.no_grad():\n",
    "            _ = model(x)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    elapsed = time.time() - start\n",
    "    total_tokens = batch_size * seq_len * n_iters\n",
    "    return total_tokens / elapsed\n",
    "\n",
    "throughput = measure_throughput(model_default)\n",
    "print(f\"Inference throughput: {throughput:,.0f} tokens/second\")\n",
    "print(f\"Device: {device}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
