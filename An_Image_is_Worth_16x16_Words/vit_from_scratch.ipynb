{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06: Vision Transformer (ViT) from Scratch for CIFAR-10\n",
    "\n",
    "Deep learning paper implementation from scratch using PyTorch.\n",
    "1. **Patch Embedding**: Split image into fixed-size patches, flatten, and project to embedding dimension\n",
    "1. Patch Embedding Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import copy\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR-10 normalization\n",
    "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
    "CIFAR10_STD = (0.2470, 0.2435, 0.2616)\n",
    "\n",
    "# Transforms with augmentation\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD),\n",
    "])\n",
    "\n",
    "# Datasets\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=train_transform\n",
    ")\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=test_transform\n",
    ")\n",
    "\n",
    "# Dataloaders\n",
    "BATCH_SIZE = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "CLASSES = ('airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Patch Embedding\n",
    "\n",
    "The patch embedding converts an image into a sequence of patch tokens:\n",
    "\n",
    "```\n",
    "Image: (B, C, H, W) = (B, 3, 32, 32)\n",
    "                \u2193\n",
    "Split into P\u00d7P patches: (B, num_patches, patch_size\u00b2\u00d7C)\n",
    "                \u2193\n",
    "Linear projection: (B, num_patches, embed_dim)\n",
    "```\n",
    "\n",
    "For a 32\u00d732 image with patch size 4: num_patches = (32/4)\u00b2 = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size: int = 32, patch_size: int = 4, \n",
    "                 in_channels: int = 3, embed_dim: int = 192):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        \n",
    "        # Use conv2d for efficient patch extraction + embedding\n",
    "        # kernel_size = stride = patch_size means no overlap\n",
    "        self.proj = nn.Conv2d(\n",
    "            in_channels, embed_dim,\n",
    "            kernel_size=patch_size, stride=patch_size\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # (B, C, H, W) -> (B, embed_dim, H/P, W/P)\n",
    "        x = self.proj(x)\n",
    "        # (B, embed_dim, H/P, W/P) -> (B, embed_dim, num_patches)\n",
    "        x = x.flatten(2)\n",
    "        # (B, embed_dim, num_patches) -> (B, num_patches, embed_dim)\n",
    "        x = x.transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Test patch embedding\n",
    "print(\"Testing PatchEmbedding...\")\n",
    "patch_embed = PatchEmbedding(img_size=32, patch_size=4, embed_dim=192)\n",
    "x = torch.randn(2, 3, 32, 32)\n",
    "patches = patch_embed(x)\n",
    "print(f\"Input: {x.shape}\")\n",
    "print(f\"Output: {patches.shape}\")\n",
    "print(f\"Number of patches: {patch_embed.num_patches}\")\n",
    "assert patches.shape == (2, 64, 192), \"Shape mismatch!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize patching\n",
    "def visualize_patches(img_tensor, patch_size=4):\n",
    "    # Unnormalize\n",
    "    img = img_tensor.numpy().transpose(1, 2, 0)\n",
    "    mean = np.array(CIFAR10_MEAN)\n",
    "    std = np.array(CIFAR10_STD)\n",
    "    img = std * img + mean\n",
    "    img = np.clip(img, 0, 1)\n",
    "    \n",
    "    h, w = img.shape[:2]\n",
    "    num_patches_h = h // patch_size\n",
    "    num_patches_w = w // patch_size\n",
    "    \n",
    "    fig, axes = plt.subplots(num_patches_h, num_patches_w + 1, \n",
    "                             figsize=(num_patches_w + 2, num_patches_h + 1))\n",
    "    \n",
    "    # Show original image in first column\n",
    "    for i in range(num_patches_h):\n",
    "        axes[i, 0].imshow(img)\n",
    "        axes[i, 0].axis('off')\n",
    "        if i == 0:\n",
    "            axes[i, 0].set_title('Original', fontsize=8)\n",
    "    \n",
    "    # Show patches\n",
    "    patch_idx = 0\n",
    "    for i in range(num_patches_h):\n",
    "        for j in range(num_patches_w):\n",
    "            patch = img[i*patch_size:(i+1)*patch_size, \n",
    "                       j*patch_size:(j+1)*patch_size]\n",
    "            axes[i, j+1].imshow(patch)\n",
    "            axes[i, j+1].axis('off')\n",
    "            axes[i, j+1].set_title(f'P{patch_idx}', fontsize=6)\n",
    "            patch_idx += 1\n",
    "    \n",
    "    plt.suptitle(f'Image split into {patch_size}\u00d7{patch_size} patches', fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Get a sample image\n",
    "sample_img, _ = test_dataset[0]\n",
    "visualize_patches(sample_img, patch_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Transformer Components\n",
    "\n",
    "### Multi-Head Self-Attention\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim: int, num_heads: int, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        \n",
    "        # Combined QKV projection for efficiency\n",
    "        self.qkv = nn.Linear(embed_dim, 3 * embed_dim, bias=True)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.proj_dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, N, D = x.shape\n",
    "        \n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, B, H, N, D/H)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # Each: (B, H, N, D/H)\n",
    "        \n",
    "        # Attention scores: (B, H, N, N)\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = self.attn_dropout(attn)\n",
    "        \n",
    "        # Apply attention to values: (B, H, N, D/H)\n",
    "        out = attn @ v\n",
    "        \n",
    "        # Concatenate heads: (B, N, D)\n",
    "        out = out.transpose(1, 2).reshape(B, N, D)\n",
    "        out = self.proj(out)\n",
    "        out = self.proj_dropout(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, embed_dim: int, mlp_ratio: float = 4.0, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        hidden_dim = int(embed_dim * mlp_ratio)\n",
    "        self.fc1 = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim: int, num_heads: int, mlp_ratio: float = 4.0, \n",
    "                 dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = MultiHeadSelfAttention(embed_dim, num_heads, dropout)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = MLP(embed_dim, mlp_ratio, dropout)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Pre-norm architecture (better for training stability)\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Vision Transformer (ViT)\n",
    "\n",
    "Full ViT architecture:\n",
    "1. Patch embedding\n",
    "2. Prepend [CLS] token\n",
    "3. Add positional embeddings\n",
    "4. Transformer encoder blocks\n",
    "5. Layer norm on [CLS] output\n",
    "6. Classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, img_size: int = 32, patch_size: int = 4, in_channels: int = 3,\n",
    "                 num_classes: int = 10, embed_dim: int = 192, depth: int = 6,\n",
    "                 num_heads: int = 6, mlp_ratio: float = 4.0, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        \n",
    "        # Patch embedding\n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
    "        \n",
    "        # Class token: learnable embedding prepended to patch sequence\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        \n",
    "        # Positional embedding: learnable, added to patch + cls tokens\n",
    "        # +1 for the cls token\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches + 1, embed_dim))\n",
    "        \n",
    "        self.pos_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Transformer encoder\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, mlp_ratio, dropout)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        \n",
    "        # Final layer norm\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # Classification head\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        # Initialize positional embedding with truncated normal\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "        \n",
    "        # Initialize other layers\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.LayerNorm):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B = x.shape[0]\n",
    "        \n",
    "        x = self.patch_embed(x)\n",
    "        \n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)  # (1, 1, D) -> (B, 1, D)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)\n",
    "        \n",
    "        # Add positional embedding\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_dropout(x)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        # Final norm\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        # Use [CLS] token for classification\n",
    "        cls_output = x[:, 0]  # (B, embed_dim)\n",
    "        logits = self.head(cls_output)  # (B, num_classes)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "def ViT_tiny(patch_size=4, **kwargs):\n",
    "    return VisionTransformer(\n",
    "        img_size=32, patch_size=patch_size, embed_dim=192,\n",
    "        depth=6, num_heads=6, mlp_ratio=4.0, **kwargs\n",
    "    )\n",
    "\n",
    "\n",
    "# Create and inspect model\n",
    "model = ViT_tiny(patch_size=4).to(device)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"ViT-Tiny parameters: {total_params:,}\")\n",
    "\n",
    "# Test forward pass\n",
    "x = torch.randn(2, 3, 32, 32).to(device)\n",
    "y = model(x)\n",
    "print(f\"Input: {x.shape} -> Output: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check 1: Patch shapes\n",
    "print(\"=\"*50)\n",
    "print(\"SANITY CHECK 1: Patch Shapes\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for patch_size in [4, 8]:\n",
    "    model = ViT_tiny(patch_size=patch_size)\n",
    "    num_patches = model.num_patches\n",
    "    expected_patches = (32 // patch_size) ** 2\n",
    "    \n",
    "    print(f\"\\nPatch size: {patch_size}\u00d7{patch_size}\")\n",
    "    print(f\"  Expected patches: {expected_patches}\")\n",
    "    print(f\"  Actual patches: {num_patches}\")\n",
    "    print(f\"  Pos embed shape: {model.pos_embed.shape}\")\n",
    "    print(f\"  \u2713 Correct!\" if num_patches == expected_patches else \"  \u2717 Mismatch!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check 2: Attention weights sum to 1\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SANITY CHECK 2: Attention Weights\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "model = ViT_tiny(patch_size=4).to(device)\n",
    "model.eval()\n",
    "\n",
    "# Hook to capture attention weights\n",
    "attn_weights = []\n",
    "def hook_fn(module, input, output):\n",
    "    # Get attention scores before softmax\n",
    "    B, N, D = input[0].shape\n",
    "    H = module.num_heads\n",
    "    head_dim = D // H\n",
    "    \n",
    "    qkv = module.qkv(input[0]).reshape(B, N, 3, H, head_dim)\n",
    "    qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "    q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "    \n",
    "    attn = (q @ k.transpose(-2, -1)) * (head_dim ** -0.5)\n",
    "    attn = F.softmax(attn, dim=-1)\n",
    "    attn_weights.append(attn.detach().cpu())\n",
    "\n",
    "# Register hook on first attention layer\n",
    "hook = model.blocks[0].attn.register_forward_hook(hook_fn)\n",
    "\n",
    "with torch.no_grad():\n",
    "    x = torch.randn(1, 3, 32, 32).to(device)\n",
    "    _ = model(x)\n",
    "\n",
    "hook.remove()\n",
    "\n",
    "# Check attention sums\n",
    "attn = attn_weights[0]  # (B, H, N, N)\n",
    "attn_sums = attn.sum(dim=-1)  # Sum over keys\n",
    "print(f\"Attention shape: {attn.shape}\")\n",
    "print(f\"Attention sums (should be 1.0): min={attn_sums.min():.6f}, max={attn_sums.max():.6f}\")\n",
    "print(f\"\u2713 Attention weights sum to 1!\" if torch.allclose(attn_sums, torch.ones_like(attn_sums), atol=1e-5) else \"\u2717 Problem!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check 3: Gradient flow\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SANITY CHECK 3: Gradient Flow\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "model = ViT_tiny(patch_size=4).to(device)\n",
    "x = torch.randn(2, 3, 32, 32, requires_grad=True).to(device)\n",
    "y = model(x)\n",
    "loss = y.sum()\n",
    "loss.backward()\n",
    "\n",
    "print(f\"Input gradient norm: {x.grad.norm().item():.4f}\")\n",
    "print(f\"Patch embed proj gradient: {model.patch_embed.proj.weight.grad.norm().item():.4f}\")\n",
    "print(f\"Pos embed gradient: {model.pos_embed.grad.norm().item():.4f}\")\n",
    "print(f\"CLS token gradient: {model.cls_token.grad.norm().item():.4f}\")\n",
    "print(f\"Block 0 attn qkv gradient: {model.blocks[0].attn.qkv.weight.grad.norm().item():.4f}\")\n",
    "print(f\"Block 5 mlp fc2 gradient: {model.blocks[5].mlp.fc2.weight.grad.norm().item():.4f}\")\n",
    "print(f\"Head gradient: {model.head.weight.grad.norm().item():.4f}\")\n",
    "print(\"\u2713 Gradients flowing through all layers!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping (helps with transformer training)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    return running_loss / total, 100. * correct / total\n",
    "\n",
    "\n",
    "def evaluate(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    return running_loss / total, 100. * correct / total\n",
    "\n",
    "\n",
    "def train_vit(model, train_loader, test_loader, epochs, lr, device, verbose=True):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # AdamW is standard for transformers\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.05, betas=(0.9, 0.999))\n",
    "    # Warmup + cosine decay\n",
    "    warmup_epochs = 10\n",
    "    \n",
    "    def lr_lambda(epoch):\n",
    "        if epoch < warmup_epochs:\n",
    "            return epoch / warmup_epochs\n",
    "        else:\n",
    "            progress = (epoch - warmup_epochs) / (epochs - warmup_epochs)\n",
    "            return 0.5 * (1 + math.cos(math.pi * progress))\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "    \n",
    "    history = {'train_loss': [], 'train_acc': [], 'test_loss': [], 'test_acc': [], 'lr': []}\n",
    "    best_acc = 0.0\n",
    "    best_state = None\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        scheduler.step()\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['test_loss'].append(test_loss)\n",
    "        history['test_acc'].append(test_acc)\n",
    "        history['lr'].append(current_lr)\n",
    "        \n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "        \n",
    "        if verbose and (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1:3d}/{epochs} | \"\n",
    "                  f\"Train: {train_acc:.2f}% | Test: {test_acc:.2f}% | LR: {current_lr:.6f}\")\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    if verbose:\n",
    "        print(f\"\\nTraining complete in {total_time:.1f}s. Best accuracy: {best_acc:.2f}%\")\n",
    "    \n",
    "    return history, best_state, best_acc, total_time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 100\n",
    "LEARNING_RATE = 1e-3  # Lower LR for transformers\n",
    "\n",
    "print(f\"Training ViT-Tiny (patch_size=4) for {NUM_EPOCHS} epochs...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "vit_model = ViT_tiny(patch_size=4, dropout=0.1).to(device)\n",
    "vit_history, vit_best_state, vit_best_acc, vit_time = train_vit(\n",
    "    vit_model, train_loader, test_loader,\n",
    "    epochs=NUM_EPOCHS, lr=LEARNING_RATE, device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(vit_history['train_loss'], label='Train')\n",
    "axes[0].plot(vit_history['test_loss'], label='Test')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Loss Curves')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(vit_history['train_acc'], label='Train')\n",
    "axes[1].plot(vit_history['test_acc'], label='Test')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy (%)')\n",
    "axes[1].set_title('Accuracy Curves')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate\n",
    "axes[2].plot(vit_history['lr'])\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('Learning Rate')\n",
    "axes[2].set_title('LR Schedule (Warmup + Cosine)')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Ablation: Patch Size 4 vs 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_patch_size_ablation(patch_size, epochs=100):\n",
    "    model = ViT_tiny(patch_size=patch_size, dropout=0.1).to(device)\n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "    num_patches = model.num_patches\n",
    "    \n",
    "    history, best_state, best_acc, train_time = train_vit(\n",
    "        model, train_loader, test_loader,\n",
    "        epochs=epochs, lr=1e-3, device=device, verbose=False\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'patch_size': patch_size,\n",
    "        'num_patches': num_patches,\n",
    "        'num_params': num_params,\n",
    "        'best_acc': best_acc,\n",
    "        'train_time': train_time,\n",
    "        'history': history\n",
    "    }\n",
    "\n",
    "print(\"Running ablation study: Patch Size 4 vs 8\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Patch size 4\n",
    "print(\"\\nTraining with patch_size=4...\")\n",
    "results_p4 = run_patch_size_ablation(patch_size=4, epochs=100)\n",
    "print(f\"  Patches: {results_p4['num_patches']}, Best accuracy: {results_p4['best_acc']:.2f}%\")\n",
    "\n",
    "# Patch size 8\n",
    "print(\"\\nTraining with patch_size=8...\")\n",
    "results_p8 = run_patch_size_ablation(patch_size=8, epochs=100)\n",
    "print(f\"  Patches: {results_p8['num_patches']}, Best accuracy: {results_p8['best_acc']:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ablation results\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ABLATION STUDY: Patch Size Comparison\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Configuration':<20} {'Patches':<12} {'Params':<15} {'Best Acc':<15} {'Time (s)':<12}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'Patch Size = 4':<20} {results_p4['num_patches']:<12} {results_p4['num_params']:>12,} {results_p4['best_acc']:>10.2f}% {results_p4['train_time']:>10.1f}\")\n",
    "print(f\"{'Patch Size = 8':<20} {results_p8['num_patches']:<12} {results_p8['num_params']:>12,} {results_p8['best_acc']:>10.2f}% {results_p8['train_time']:>10.1f}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "diff = results_p4['best_acc'] - results_p8['best_acc']\n",
    "speedup = results_p4['train_time'] / results_p8['train_time']\n",
    "print(f\"\\nPatch size 4 vs 8:\")\n",
    "print(f\"  Accuracy difference: {diff:+.2f}%\")\n",
    "print(f\"  Patch 8 speedup: {speedup:.2f}x faster\")\n",
    "print(f\"  Patch 8 has {(results_p4['num_patches'] / results_p8['num_patches']):.1f}x fewer patches (shorter sequence)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ablation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Test accuracy\n",
    "axes[0].plot(results_p4['history']['test_acc'], label='Patch=4 (64 patches)', alpha=0.8)\n",
    "axes[0].plot(results_p8['history']['test_acc'], label='Patch=8 (16 patches)', alpha=0.8)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Test Accuracy (%)')\n",
    "axes[0].set_title('Test Accuracy: Patch Size Comparison')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Bar comparison\n",
    "configs = ['Patch=4\\n(64 patches)', 'Patch=8\\n(16 patches)']\n",
    "accs = [results_p4['best_acc'], results_p8['best_acc']]\n",
    "times = [results_p4['train_time'], results_p8['train_time']]\n",
    "\n",
    "x = np.arange(len(configs))\n",
    "width = 0.35\n",
    "\n",
    "ax2 = axes[1]\n",
    "bars1 = ax2.bar(x - width/2, accs, width, label='Accuracy (%)', color='#4ecdc4')\n",
    "ax2.set_ylabel('Accuracy (%)', color='#4ecdc4')\n",
    "ax2.tick_params(axis='y', labelcolor='#4ecdc4')\n",
    "\n",
    "ax3 = ax2.twinx()\n",
    "bars2 = ax3.bar(x + width/2, times, width, label='Time (s)', color='#ff6b6b')\n",
    "ax3.set_ylabel('Training Time (s)', color='#ff6b6b')\n",
    "ax3.tick_params(axis='y', labelcolor='#ff6b6b')\n",
    "\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(configs)\n",
    "ax2.set_title('Accuracy vs Training Time')\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars1, accs):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "             f'{val:.1f}%', ha='center', fontsize=9, color='#4ecdc4')\n",
    "for bar, val in zip(bars2, times):\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5,\n",
    "             f'{val:.0f}s', ha='center', fontsize=9, color='#ff6b6b')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}