{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04: CNN Baseline for CIFAR-10 from Scratch\n",
    "\n",
    "Deep learning paper implementation from scratch using PyTorch.\n",
    "1. Data Loading with Augmentations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Pipeline with Augmentations\n",
    "\n",
    "CIFAR-10 consists of 60,000 32x32 color images in 10 classes:\n",
    "- airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck\n",
    "\n",
    "We use standard augmentations:\n",
    "- Random horizontal flip\n",
    "- Random crop with padding\n",
    "- Normalization with CIFAR-10 statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR-10 normalization values\n",
    "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
    "CIFAR10_STD = (0.2470, 0.2435, 0.2616)\n",
    "\n",
    "# Training transforms with augmentation\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD),\n",
    "])\n",
    "\n",
    "# Test transforms (no augmentation)\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD),\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=train_transform\n",
    ")\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=test_transform\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True\n",
    ")\n",
    "\n",
    "# Class names\n",
    "CLASSES = ('airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Training batches per epoch: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some training samples\n",
    "def imshow(img, title=None):\n",
    "    img = img.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array(CIFAR10_MEAN)\n",
    "    std = np.array(CIFAR10_STD)\n",
    "    img = std * img + mean\n",
    "    img = np.clip(img, 0, 1)\n",
    "    plt.imshow(img)\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.axis('off')\n",
    "\n",
    "# Get a batch\n",
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(2, 8, figsize=(14, 4))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    plt.sca(ax)\n",
    "    imshow(images[i])\n",
    "    ax.set_title(CLASSES[labels[i]], fontsize=9)\n",
    "plt.suptitle('Sample Training Images (with augmentation)', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CNN Architecture\n",
    "\n",
    "A simple but effective CNN architecture:\n",
    "- 3 convolutional blocks, each with Conv -> BatchNorm -> ReLU -> MaxPool\n",
    "- Increasing channels: 32 -> 64 -> 128\n",
    "- Global average pooling before classifier\n",
    "- Optional dropout for regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, pool: bool = True):\n",
    "        super().__init__()\n",
    "        layers = [\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ]\n",
    "        if pool:\n",
    "            layers.append(nn.MaxPool2d(2, 2))\n",
    "        self.block = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes: int = 10, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            ConvBlock(3, 32, pool=True),      # 32x16x16\n",
    "            ConvBlock(32, 64, pool=True),     # 64x8x8\n",
    "            ConvBlock(64, 128, pool=True),    # 128x4x4\n",
    "            ConvBlock(128, 256, pool=False),  # 256x4x4\n",
    "        )\n",
    "        \n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
    "        self.classifier = nn.Linear(256, num_classes)\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.features(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Create model and inspect\n",
    "model = SimpleCNN(num_classes=10, dropout=0.0).to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Test forward pass\n",
    "test_input = torch.randn(2, 3, 32, 32).to(device)\n",
    "test_output = model(test_input)\n",
    "print(f\"\\nInput shape: {test_input.shape}\")\n",
    "print(f\"Output shape: {test_output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Loop with Checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100. * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def evaluate(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    test_loss = running_loss / total\n",
    "    test_acc = 100. * correct / total\n",
    "    return test_loss, test_acc\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, test_loader, epochs, lr, device, verbose=True):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [], 'train_acc': [],\n",
    "        'test_loss': [], 'test_acc': [],\n",
    "        'lr': []\n",
    "    }\n",
    "    \n",
    "    best_acc = 0.0\n",
    "    best_model_state = None\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        \n",
    "        test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "        \n",
    "        # Get current LR\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # Update scheduler\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Record history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['test_loss'].append(test_loss)\n",
    "        history['test_acc'].append(test_acc)\n",
    "        history['lr'].append(current_lr)\n",
    "        \n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "        \n",
    "        if verbose and (epoch + 1) % 5 == 0:\n",
    "            print(f\"Epoch {epoch+1:3d}/{epochs} | \"\n",
    "                  f\"Train Loss: {train_loss:.4f}, Acc: {train_acc:.2f}% | \"\n",
    "                  f\"Test Loss: {test_loss:.4f}, Acc: {test_acc:.2f}% | \"\n",
    "                  f\"LR: {current_lr:.6f}\")\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nTraining complete in {total_time:.1f}s\")\n",
    "        print(f\"Best test accuracy: {best_acc:.2f}%\")\n",
    "    \n",
    "    return history, best_model_state, best_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "NUM_EPOCHS = 50\n",
    "LEARNING_RATE = 0.1\n",
    "\n",
    "print(f\"Training SimpleCNN for {NUM_EPOCHS} epochs...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "model = SimpleCNN(num_classes=10, dropout=0.0).to(device)\n",
    "history, best_state, best_acc = train_model(\n",
    "    model, train_loader, test_loader, \n",
    "    epochs=NUM_EPOCHS, lr=LEARNING_RATE, device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history['train_loss'], label='Train')\n",
    "axes[0].plot(history['test_loss'], label='Test')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Loss Curves')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(history['train_acc'], label='Train')\n",
    "axes[1].plot(history['test_acc'], label='Test')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy (%)')\n",
    "axes[1].set_title('Accuracy Curves')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate\n",
    "axes[2].plot(history['lr'])\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('Learning Rate')\n",
    "axes[2].set_title('Learning Rate Schedule (Cosine Annealing)')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model and evaluate\n",
    "model.load_state_dict(best_state)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "print(f\"Final Test Results (Best Model):\")\n",
    "print(f\"  Loss: {test_loss:.4f}\")\n",
    "print(f\"  Accuracy: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class accuracy\n",
    "def get_class_accuracy(model, test_loader, device, num_classes=10):\n",
    "    model.eval()\n",
    "    class_correct = [0] * num_classes\n",
    "    class_total = [0] * num_classes\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            \n",
    "            for i in range(targets.size(0)):\n",
    "                label = targets[i].item()\n",
    "                class_total[label] += 1\n",
    "                if predicted[i] == label:\n",
    "                    class_correct[label] += 1\n",
    "    \n",
    "    return {CLASSES[i]: 100 * class_correct[i] / class_total[i] for i in range(num_classes)}\n",
    "\n",
    "class_acc = get_class_accuracy(model, test_loader, device)\n",
    "\n",
    "print(\"\\nPer-class accuracy:\")\n",
    "print(\"-\" * 30)\n",
    "for cls, acc in sorted(class_acc.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {cls:12s}: {acc:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some predictions\n",
    "model.eval()\n",
    "images, labels = next(iter(test_loader))\n",
    "images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(images)\n",
    "    _, preds = outputs.max(1)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(2, 8, figsize=(14, 4))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    plt.sca(ax)\n",
    "    img = images[i].cpu()\n",
    "    imshow(img)\n",
    "    color = 'green' if preds[i] == labels[i] else 'red'\n",
    "    ax.set_title(f'{CLASSES[preds[i]]}', fontsize=9, color=color)\n",
    "plt.suptitle('Predictions (green=correct, red=wrong)', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Ablation: Dropout ON vs OFF\n",
    "\n",
    "Compare model performance with and without dropout regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ablation(dropout_rate, epochs=50, lr=0.1):\n",
    "    model = SimpleCNN(num_classes=10, dropout=dropout_rate).to(device)\n",
    "    history, best_state, best_acc = train_model(\n",
    "        model, train_loader, test_loader,\n",
    "        epochs=epochs, lr=lr, device=device, verbose=False\n",
    "    )\n",
    "    \n",
    "    # Load best and get final test accuracy\n",
    "    model.load_state_dict(best_state)\n",
    "    _, final_acc = evaluate(model, test_loader, nn.CrossEntropyLoss(), device)\n",
    "    \n",
    "    return {\n",
    "        'dropout': dropout_rate,\n",
    "        'best_acc': best_acc,\n",
    "        'final_acc': final_acc,\n",
    "        'history': history\n",
    "    }\n",
    "\n",
    "print(\"Running ablation study: Dropout ON vs OFF\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Dropout OFF\n",
    "print(\"\\nTraining with dropout=0.0...\")\n",
    "results_no_dropout = run_ablation(dropout_rate=0.0, epochs=50)\n",
    "print(f\"  Best accuracy: {results_no_dropout['best_acc']:.2f}%\")\n",
    "\n",
    "# Dropout ON\n",
    "print(\"\\nTraining with dropout=0.5...\")\n",
    "results_dropout = run_ablation(dropout_rate=0.5, epochs=50)\n",
    "print(f\"  Best accuracy: {results_dropout['best_acc']:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ablation results table\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ABLATION STUDY: Dropout Comparison\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Configuration':<20} {'Best Test Acc':<20} {'Final Test Acc':<20}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'Dropout=0.0 (OFF)':<20} {results_no_dropout['best_acc']:>15.2f}% {results_no_dropout['final_acc']:>15.2f}%\")\n",
    "print(f\"{'Dropout=0.5 (ON)':<20} {results_dropout['best_acc']:>15.2f}% {results_dropout['final_acc']:>15.2f}%\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "diff = results_dropout['best_acc'] - results_no_dropout['best_acc']\n",
    "winner = \"Dropout ON\" if diff > 0 else \"Dropout OFF\"\n",
    "print(f\"\\nDifference: {abs(diff):.2f}% ({'better' if diff > 0 else 'worse'} with dropout)\")\n",
    "print(f\"Winner: {winner}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ablation comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Test accuracy curves\n",
    "axes[0].plot(results_no_dropout['history']['test_acc'], label='Dropout=0.0', alpha=0.8)\n",
    "axes[0].plot(results_dropout['history']['test_acc'], label='Dropout=0.5', alpha=0.8)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Test Accuracy (%)')\n",
    "axes[0].set_title('Test Accuracy: Dropout Comparison')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Train-Test gap (generalization)\n",
    "gap_no_dropout = [t - v for t, v in zip(results_no_dropout['history']['train_acc'], \n",
    "                                         results_no_dropout['history']['test_acc'])]\n",
    "gap_dropout = [t - v for t, v in zip(results_dropout['history']['train_acc'], \n",
    "                                      results_dropout['history']['test_acc'])]\n",
    "\n",
    "axes[1].plot(gap_no_dropout, label='Dropout=0.0', alpha=0.8)\n",
    "axes[1].plot(gap_dropout, label='Dropout=0.5', alpha=0.8)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Train - Test Accuracy (%)')\n",
    "axes[1].set_title('Generalization Gap (lower is better)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal generalization gap:\")\n",
    "print(f\"  Dropout=0.0: {gap_no_dropout[-1]:.2f}%\")\n",
    "print(f\"  Dropout=0.5: {gap_dropout[-1]:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}