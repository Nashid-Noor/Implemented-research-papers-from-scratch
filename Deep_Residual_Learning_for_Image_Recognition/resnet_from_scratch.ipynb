{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05: ResNet from Scratch for CIFAR-10\n",
    "\n",
    "Deep learning paper implementation from scratch using PyTorch.\n",
    "1. BasicBlock Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR-10 normalization\n",
    "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
    "CIFAR10_STD = (0.2470, 0.2435, 0.2616)\n",
    "\n",
    "# Transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD),\n",
    "])\n",
    "\n",
    "# Datasets\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=train_transform\n",
    ")\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=test_transform\n",
    ")\n",
    "\n",
    "# Dataloaders\n",
    "BATCH_SIZE = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "CLASSES = ('airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. BasicBlock Implementation\n",
    "\n",
    "The BasicBlock is the fundamental building unit of ResNet-18/34:\n",
    "\n",
    "```\n",
    "Input (x)\n",
    "    \u2502\n",
    "    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "    \u2502                      \u2502 (identity or projection)\n",
    "    \u25bc                      \u2502\n",
    "Conv 3x3 \u2192 BN \u2192 ReLU       \u2502\n",
    "    \u2502                      \u2502\n",
    "    \u25bc                      \u2502\n",
    "Conv 3x3 \u2192 BN              \u2502\n",
    "    \u2502                      \u2502\n",
    "    \u25bc                      \u25bc\n",
    "    \u2514\u2500\u2500\u2500\u2500\u2500\u2500 + \u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "            \u2502\n",
    "            \u25bc\n",
    "          ReLU\n",
    "            \u2502\n",
    "            \u25bc\n",
    "         Output\n",
    "```\n",
    "\n",
    "When stride > 1 or channels change, we need a **projection shortcut** (1x1 conv) to match dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1  # Output channels = in_channels * expansion\n",
    "    \n",
    "    def __init__(self, in_channels: int, out_channels: int, stride: int = 1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Main path\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels, out_channels, kernel_size=3, \n",
    "            stride=stride, padding=1, bias=False\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(\n",
    "            out_channels, out_channels, kernel_size=3,\n",
    "            stride=1, padding=1, bias=False\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # Shortcut (identity or projection)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            # Projection shortcut: 1x1 conv to match dimensions\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, \n",
    "                          stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Main path\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        \n",
    "        # Skip connection\n",
    "        out = out + self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "# Test BasicBlock\n",
    "print(\"Testing BasicBlock...\")\n",
    "\n",
    "# Identity shortcut (same channels, stride=1)\n",
    "block1 = BasicBlock(64, 64, stride=1)\n",
    "x = torch.randn(2, 64, 16, 16)\n",
    "y = block1(x)\n",
    "print(f\"Identity: {x.shape} -> {y.shape}\")\n",
    "\n",
    "# Projection shortcut (different channels)\n",
    "block2 = BasicBlock(64, 128, stride=1)\n",
    "y = block2(x)\n",
    "print(f\"Channel change: {x.shape} -> {y.shape}\")\n",
    "\n",
    "# Downsampling (stride=2)\n",
    "block3 = BasicBlock(64, 128, stride=2)\n",
    "y = block3(x)\n",
    "print(f\"Downsample: {x.shape} -> {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ResNet Architecture\n",
    "\n",
    "ResNet-18 structure for CIFAR-10 (adapted for 32x32 input):\n",
    "- Initial conv: 3x3, 64 channels (no 7x7 or maxpool since images are small)\n",
    "- Layer 1: 2 BasicBlocks, 64 channels\n",
    "- Layer 2: 2 BasicBlocks, 128 channels, stride 2\n",
    "- Layer 3: 2 BasicBlocks, 256 channels, stride 2\n",
    "- Layer 4: 2 BasicBlocks, 512 channels, stride 2\n",
    "- Global average pool + FC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks: list, num_classes: int = 10, \n",
    "                 base_channels: int = 64):\n",
    "        super().__init__()\n",
    "        self.in_channels = base_channels\n",
    "        \n",
    "        # Initial convolution (3x3 for CIFAR)\n",
    "        self.conv1 = nn.Conv2d(3, base_channels, kernel_size=3, \n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(base_channels)\n",
    "        \n",
    "        # Residual layers\n",
    "        self.layer1 = self._make_layer(block, base_channels, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, base_channels * 2, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, base_channels * 4, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, base_channels * 8, num_blocks[3], stride=2)\n",
    "        \n",
    "        # Classifier\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(base_channels * 8 * block.expansion, num_classes)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _make_layer(self, block, out_channels: int, num_blocks: int, stride: int):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        \n",
    "        for s in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, s))\n",
    "            self.in_channels = out_channels * block.expansion\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Initial conv\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        \n",
    "        # Residual layers\n",
    "        out = self.layer1(out)  # 32x32\n",
    "        out = self.layer2(out)  # 16x16\n",
    "        out = self.layer3(out)  # 8x8\n",
    "        out = self.layer4(out)  # 4x4\n",
    "        \n",
    "        # Classifier\n",
    "        out = self.avgpool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18(num_classes=10):\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes=num_classes)\n",
    "\n",
    "\n",
    "# Create and inspect model\n",
    "model = ResNet18().to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"ResNet-18 parameters: {total_params:,}\")\n",
    "\n",
    "# Test forward pass\n",
    "x = torch.randn(2, 3, 32, 32).to(device)\n",
    "y = model(x)\n",
    "print(f\"Input: {x.shape} -> Output: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify skip connections work (gradient flow)\n",
    "print(\"\\nVerifying gradient flow through skip connections...\")\n",
    "\n",
    "model = ResNet18().to(device)\n",
    "x = torch.randn(2, 3, 32, 32, requires_grad=True).to(device)\n",
    "y = model(x)\n",
    "loss = y.sum()\n",
    "loss.backward()\n",
    "\n",
    "# Check gradients in different layers\n",
    "print(f\"Input gradient norm: {x.grad.norm().item():.4f}\")\n",
    "print(f\"Conv1 weight gradient norm: {model.conv1.weight.grad.norm().item():.4f}\")\n",
    "print(f\"Layer4 block0 conv1 gradient norm: {model.layer4[0].conv1.weight.grad.norm().item():.4f}\")\n",
    "print(\"Gradients are flowing through all layers!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    return running_loss / total, 100. * correct / total\n",
    "\n",
    "\n",
    "def evaluate(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    return running_loss / total, 100. * correct / total\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, test_loader, epochs, lr, weight_decay, \n",
    "                device, scheduler_type='cosine', verbose=True):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "    \n",
    "    if scheduler_type == 'cosine':\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    else:  # step\n",
    "        scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[epochs//2, 3*epochs//4], gamma=0.1)\n",
    "    \n",
    "    history = {'train_loss': [], 'train_acc': [], 'test_loss': [], 'test_acc': [], 'lr': []}\n",
    "    best_acc = 0.0\n",
    "    best_state = None\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        scheduler.step()\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['test_loss'].append(test_loss)\n",
    "        history['test_acc'].append(test_acc)\n",
    "        history['lr'].append(current_lr)\n",
    "        \n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "        \n",
    "        if verbose and (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1:3d}/{epochs} | \"\n",
    "                  f\"Train: {train_acc:.2f}% | Test: {test_acc:.2f}% | LR: {current_lr:.6f}\")\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    if verbose:\n",
    "        print(f\"\\nTraining complete in {total_time:.1f}s. Best accuracy: {best_acc:.2f}%\")\n",
    "    \n",
    "    return history, best_state, best_acc, total_time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train ResNet-18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 100\n",
    "LEARNING_RATE = 0.1\n",
    "WEIGHT_DECAY = 5e-4\n",
    "\n",
    "print(f\"Training ResNet-18 for {NUM_EPOCHS} epochs...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "resnet_model = ResNet18().to(device)\n",
    "resnet_history, resnet_best_state, resnet_best_acc, resnet_time = train_model(\n",
    "    resnet_model, train_loader, test_loader,\n",
    "    epochs=NUM_EPOCHS, lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY,\n",
    "    device=device, scheduler_type='cosine'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparison: Simple CNN vs ResNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple CNN baseline (from notebook 04)\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.classifier(self.features(x))\n",
    "\n",
    "# Train CNN for quick comparison (fewer epochs)\n",
    "print(\"Training Simple CNN baseline (50 epochs for comparison)...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "cnn_model = SimpleCNN().to(device)\n",
    "cnn_history, cnn_best_state, cnn_best_acc, cnn_time = train_model(\n",
    "    cnn_model, train_loader, test_loader,\n",
    "    epochs=50, lr=0.1, weight_decay=5e-4,\n",
    "    device=device, scheduler_type='cosine'\n",
    ")\n",
    "\n",
    "cnn_params = sum(p.numel() for p in cnn_model.parameters())\n",
    "resnet_params = sum(p.numel() for p in resnet_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison table\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL COMPARISON: Simple CNN vs ResNet-18\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Metric':<25} {'Simple CNN':<20} {'ResNet-18':<20}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'Parameters':<25} {cnn_params:>15,} {resnet_params:>15,}\")\n",
    "print(f\"{'Training Epochs':<25} {50:>15} {NUM_EPOCHS:>15}\")\n",
    "print(f\"{'Best Test Accuracy':<25} {cnn_best_acc:>14.2f}% {resnet_best_acc:>14.2f}%\")\n",
    "print(f\"{'Training Time (s)':<25} {cnn_time:>15.1f} {resnet_time:>15.1f}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "improvement = resnet_best_acc - cnn_best_acc\n",
    "print(f\"\\nResNet-18 improvement over CNN: {improvement:+.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Test accuracy\n",
    "axes[0].plot(cnn_history['test_acc'], label='Simple CNN (50 ep)', alpha=0.8)\n",
    "axes[0].plot(resnet_history['test_acc'], label=f'ResNet-18 ({NUM_EPOCHS} ep)', alpha=0.8)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Test Accuracy (%)')\n",
    "axes[0].set_title('Test Accuracy Comparison')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Bar chart comparison\n",
    "models = ['Simple CNN', 'ResNet-18']\n",
    "accs = [cnn_best_acc, resnet_best_acc]\n",
    "colors = ['#ff6b6b', '#4ecdc4']\n",
    "\n",
    "bars = axes[1].bar(models, accs, color=colors)\n",
    "axes[1].set_ylabel('Best Test Accuracy (%)')\n",
    "axes[1].set_title('Best Accuracy Comparison')\n",
    "axes[1].set_ylim([min(accs) - 5, max(accs) + 5])\n",
    "\n",
    "for bar, acc in zip(bars, accs):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                 f'{acc:.2f}%', ha='center', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Ablation: Weight Decay 5e-4 vs 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_weight_decay_ablation(weight_decay, epochs=100):\n",
    "    model = ResNet18().to(device)\n",
    "    history, best_state, best_acc, train_time = train_model(\n",
    "        model, train_loader, test_loader,\n",
    "        epochs=epochs, lr=0.1, weight_decay=weight_decay,\n",
    "        device=device, scheduler_type='cosine', verbose=False\n",
    "    )\n",
    "    return {\n",
    "        'weight_decay': weight_decay,\n",
    "        'best_acc': best_acc,\n",
    "        'train_time': train_time,\n",
    "        'history': history\n",
    "    }\n",
    "\n",
    "print(\"Running ablation study: Weight Decay comparison\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Weight decay = 5e-4 (standard)\n",
    "print(\"\\nTraining with weight_decay=5e-4...\")\n",
    "results_wd5e4 = run_weight_decay_ablation(5e-4, epochs=100)\n",
    "print(f\"  Best accuracy: {results_wd5e4['best_acc']:.2f}%\")\n",
    "\n",
    "# Weight decay = 1e-4\n",
    "print(\"\\nTraining with weight_decay=1e-4...\")\n",
    "results_wd1e4 = run_weight_decay_ablation(1e-4, epochs=100)\n",
    "print(f\"  Best accuracy: {results_wd1e4['best_acc']:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ablation results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ABLATION STUDY: Weight Decay Comparison\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Configuration':<25} {'Best Test Acc':<20} {'Train Time (s)':<20}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'Weight Decay = 5e-4':<25} {results_wd5e4['best_acc']:>15.2f}% {results_wd5e4['train_time']:>15.1f}\")\n",
    "print(f\"{'Weight Decay = 1e-4':<25} {results_wd1e4['best_acc']:>15.2f}% {results_wd1e4['train_time']:>15.1f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "diff = results_wd5e4['best_acc'] - results_wd1e4['best_acc']\n",
    "better = \"5e-4\" if diff > 0 else \"1e-4\"\n",
    "print(f\"\\nDifference: {abs(diff):.2f}%\")\n",
    "print(f\"Better weight decay: {better}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ablation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Test accuracy\n",
    "axes[0].plot(results_wd5e4['history']['test_acc'], label='WD=5e-4', alpha=0.8)\n",
    "axes[0].plot(results_wd1e4['history']['test_acc'], label='WD=1e-4', alpha=0.8)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Test Accuracy (%)')\n",
    "axes[0].set_title('Test Accuracy: Weight Decay Comparison')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Train-test gap\n",
    "gap_5e4 = [t - v for t, v in zip(results_wd5e4['history']['train_acc'], results_wd5e4['history']['test_acc'])]\n",
    "gap_1e4 = [t - v for t, v in zip(results_wd1e4['history']['train_acc'], results_wd1e4['history']['test_acc'])]\n",
    "\n",
    "axes[1].plot(gap_5e4, label='WD=5e-4', alpha=0.8)\n",
    "axes[1].plot(gap_1e4, label='WD=1e-4', alpha=0.8)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Train - Test Accuracy (%)')\n",
    "axes[1].set_title('Generalization Gap')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final generalization gap:\")\n",
    "print(f\"  WD=5e-4: {gap_5e4[-1]:.2f}%\")\n",
    "print(f\"  WD=1e-4: {gap_1e4[-1]:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}